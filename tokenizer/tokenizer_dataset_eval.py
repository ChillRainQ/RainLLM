import os
import json
import math
import re
import random
import shutil
import time
from collections import Counter, defaultdict
from pathlib import Path
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from transformers import AutoTokenizer
from scipy.stats import entropy


class TokenizerEvaluator:
    def __init__(self, tokenizer_path, new_data_path, sample_size=1000000, shuffle_sample_size=10000):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨è¯„ä¼°å™¨

        å‚æ•°:
        tokenizer_path: é¢„è®­ç»ƒåˆ†è¯å™¨è·¯å¾„ (æœ¬åœ°æˆ–Hugging Faceæ¨¡å‹ID)
        new_data_path: æ–°æ•°æ®é›†è·¯å¾„ (ç›®å½•æˆ–æ–‡ä»¶)
        sample_size: åˆ†è¯åˆ†æé‡‡æ ·å¤§å° (å­—ç¬¦æ•°)
        shuffle_sample_size: æ‰“ä¹±éªŒè¯é‡‡æ ·è¡Œæ•°
        """
        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
        self.new_data_path = new_data_path
        self.sample_size = sample_size
        self.shuffle_sample_size = shuffle_sample_size
        self.original_vocab = set(self.tokenizer.get_vocab().keys())
        self.results = {}

        # ç‰¹æ®Šæ ‡è®°å¤„ç†
        self.special_tokens = set(self.tokenizer.all_special_tokens)
        self.unk_token = self.tokenizer.unk_token

    def validate_shuffle(self):
        """éªŒè¯æ•°æ®é›†æ˜¯å¦å……åˆ†æ‰“ä¹±ï¼ˆä¼˜åŒ–æƒ…æ„Ÿå¯¹è¯æ£€æµ‹ï¼‰"""
        if not self.new_data_path.endswith('.jsonl'):
            print("âš ï¸ æ‰“ä¹±éªŒè¯ä»…æ”¯æŒJSONLæ ¼å¼æ–‡ä»¶")
            return False

        print("â–¶ éªŒè¯æ•°æ®é›†æ‰“ä¹±è´¨é‡...")
        topic_sequence = []
        topic_count = defaultdict(int)

        # æ‰©å±•ä¸»é¢˜å…³é”®è¯ï¼ˆå¢åŠ æƒ…æ„Ÿå¯¹è¯ç±»åˆ«ï¼‰
        topic_keywords = {
            "medical": ["åŒ»å­¦", "åŒ»ç–—", "å¥åº·", "ç–¾ç—…", "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‚£è€…", "æ²»ç–—", "è¯Šæ–­", "è¯ç‰©", "æ‰‹æœ¯"],
            "code": ["ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "å˜é‡", "ç±»", "æ–¹æ³•", "ç®—æ³•", "æ•°æ®åº“", "API", "ç¼–ç¨‹è¯­è¨€", "è°ƒè¯•", "æ¡†æ¶"],
            "finance": ["é‡‘è", "è‚¡ç¥¨", "æŠ•èµ„", "é“¶è¡Œ", "ç»æµ", "å¸‚åœº", "äº¤æ˜“", "è´§å¸", "æ±‡ç‡", "åŸºé‡‘", "ç†è´¢"],
            "education": ["æ•™è‚²", "å­¦æ ¡", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™å­¦", "è€å¸ˆ", "å­¦ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "æ•™æ", "å­¦ä½"],
            "tech": ["æŠ€æœ¯", "ç§‘æŠ€", "åˆ›æ–°", "ç ”å‘", "å·¥ç¨‹", "è®¾å¤‡", "ç³»ç»Ÿ", "åº”ç”¨", "æ™ºèƒ½", "æœºå™¨äºº", "äººå·¥æ™ºèƒ½"],
            "entertainment": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ¸¸æˆ", "æ˜æ˜Ÿ", "èŠ‚ç›®", "æ¼”å‡º", "ç»¼è‰º", "è‰ºäºº", "æ¼”å”±ä¼š",
                              "ç”µè§†å‰§"],
            "emotional": ["æƒ…æ„Ÿ", "å¿ƒæƒ…", "æ„Ÿå—", "çˆ±", "å–œæ¬¢", "æ‹çˆ±", "åˆ†æ‰‹", "å®‰æ…°", "æ”¯æŒ", "ç†è§£", "å…³ç³»",
                          "æœ‹å‹", "å®¶äºº", "å­¤ç‹¬", "å¼€å¿ƒ", "ä¼¤å¿ƒ", "æ„¤æ€’", "å‹åŠ›", "å¿ƒç†å’¨è¯¢", "æƒ…ç»ªç®¡ç†", "å…±æƒ…",
                          "äº²å¯†å…³ç³»", "æƒ…æ„Ÿè¡¨è¾¾", "å¿ƒç†æ”¯æŒ", "æƒ…æ„Ÿéœ€æ±‚", "æƒ…æ„Ÿå›°æƒ‘", "æƒ…æ„Ÿäº¤æµ", "æƒ…æ„Ÿå’¨è¯¢"]
        }

        # æƒ…æ„Ÿå¯¹è¯ä¸“ç”¨æ£€æµ‹æ¨¡å¼ - æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
        emotional_patterns = [
            r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(æ„Ÿåˆ°|è§‰å¾—|æ„Ÿè§‰|è®¤ä¸º|ä»¥ä¸º|å‘ç°|çŸ¥é“|å¸Œæœ›|æƒ³è¦|éœ€è¦|å–œæ¬¢|çˆ±|è®¨åŒ|æ¨|å®³æ€•|æ‹…å¿ƒ|ç„¦è™‘|ç”Ÿæ°”|æ„¤æ€’|ä¼¤å¿ƒ|éš¾è¿‡|å¼€å¿ƒ|å¿«ä¹|å…´å¥‹|æƒŠå–œ|å‹åŠ›å¤§|å­¤ç‹¬|å¯‚å¯|æ²®ä¸§|å¤±æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«|çƒ¦æ¼|éƒé—·|å§”å±ˆ|å«‰å¦’|ç¾¡æ…•|å†…ç–š|åæ‚”|ç¾æ„§|è‡ªè±ª|æ»¡è¶³|æ”¾æ¾|å¹³é™|å®‰å¿ƒ|æ„Ÿæ¿€|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|èˆ’æœ|è‡ªåœ¨|è‡ªä¿¡|ä¹è§‚|æ‚²è§‚|æ¶ˆæ|ç§¯æ|ç´§å¼ |ç–²åŠ³|ç´¯|å›°|é¥¿|æ¸´|ç—›|ç—’|å†·|çƒ­|ä¸èˆ’æœ)?[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|å«‰å¦’|ç¾¡æ…•|äº²å¯†|ç–è¿œ|å†·æ·¡|çƒ­æƒ…|å…³å¿ƒ|ä½“è´´|ç†è§£|æ”¯æŒ|å®‰æ…°|é¼“åŠ±|å¸®åŠ©|é™ªä¼´|å€¾å¬|å€¾è¯‰|åˆ†äº«|æ²Ÿé€š|äº¤æµ|è¡¨è¾¾|æ²‰é»˜|å†·æˆ˜|å†²çª|çŸ›ç›¾|è§£å†³|å¤„ç†|åº”å¯¹|è°ƒèŠ‚|æ§åˆ¶|é‡Šæ”¾|å‘æ³„|å‹æŠ‘|ç§¯ç´¯|çˆ†å‘|æ¢å¤|é‡å»º|ç»´ç³»|ç»è¥|çæƒœ|æ”¾å¼ƒ|æ”¾ä¸‹|å¿˜è®°|æ€€å¿µ|æ€å¿µ|å›å¿†|è¿‡å»|ç°åœ¨|æœªæ¥|å®¶åº­|çˆ¶æ¯|å­å¥³|å…„å¼Ÿå§å¦¹|äº²æˆš|æœ‹å‹|é—ºèœœ|å“¥ä»¬|åŒäº‹|åŒå­¦|æ‹äºº|çˆ±äºº|ä¼´ä¾£|å¤«å¦»|æƒ…ä¾£|å•èº«|æ‹çˆ±å…³ç³»|å©šå§»å…³ç³»|äº²å­å…³ç³»|å‹æƒ…|äººé™…å…³ç³»|ç¤¾äº¤|å­¤ç‹¬|åˆç¾¤|æ’æ–¥|æ¥çº³|è®¤åŒ|å°Šé‡|åŒ…å®¹|å®½å®¹|ä½“è°…|å…³å¿ƒ|çˆ±æŠ¤|ä¿æŠ¤|ä¾èµ–|ç‹¬ç«‹|è‡ªç”±|æŸç¼š|æ§åˆ¶|å æœ‰|ç‰ºç‰²|ä»˜å‡º|å›æŠ¥|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|ç—›è‹¦|å¿«ä¹)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
            r"(å¦‚ä½•|æ€æ ·|æ€ä¹ˆ|ä»€ä¹ˆ|ä¸ºä»€ä¹ˆ|æ˜¯ä¸æ˜¯|èƒ½å¦|è¦ä¸è¦|è¯¥ä¸è¯¥|å€¼ä¸å€¼|æœ‰æ²¡æœ‰|èƒ½ä¸èƒ½|ä¼šä¸ä¼š|æƒ³ä¸æƒ³|æ„¿ä¸æ„¿|æ•¢ä¸æ•¢|è‚¯ä¸è‚¯)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(å¤„ç†|åº”å¯¹|è§£å†³|é¢å¯¹|çœ‹å¾…|ç†è§£|åˆ†æ|è°ƒèŠ‚|æ§åˆ¶|é‡Šæ”¾|è¡¨è¾¾|æ²Ÿé€š|äº¤æµ|æ”¹å–„|ä¿®å¤|ç»´ç³»|ç»è¥|çæƒœ|æ”¾å¼ƒ|æ”¾ä¸‹|å¿˜è®°|æ€€å¿µ|å›å¿†|é‡å»º|æ¢å¤)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|å«‰å¦’|ç¾¡æ…•|äº²å¯†|ç–è¿œ|å†·æ·¡|çƒ­æƒ…|å…³å¿ƒ|ä½“è´´|ç†è§£|æ”¯æŒ|å®‰æ…°|é¼“åŠ±|å¸®åŠ©|é™ªä¼´|å€¾å¬|å€¾è¯‰|åˆ†äº«|æ²Ÿé€š|äº¤æµ|è¡¨è¾¾|æ²‰é»˜|å†·æˆ˜|å†²çª|çŸ›ç›¾|å…³ç³»|å®¶åº­|çˆ¶æ¯|å­å¥³|å…„å¼Ÿå§å¦¹|äº²æˆš|æœ‹å‹|é—ºèœœ|å“¥ä»¬|åŒäº‹|åŒå­¦|æ‹äºº|çˆ±äºº|ä¼´ä¾£|å¤«å¦»|æƒ…ä¾£|å•èº«|æ‹çˆ±å…³ç³»|å©šå§»å…³ç³»|äº²å­å…³ç³»|å‹æƒ…|äººé™…å…³ç³»|ç¤¾äº¤|å­¤ç‹¬|åˆç¾¤|æ’æ–¥|æ¥çº³|è®¤åŒ|å°Šé‡|åŒ…å®¹|å®½å®¹|ä½“è°…|å…³å¿ƒ|çˆ±æŠ¤|ä¿æŠ¤|ä¾èµ–|ç‹¬ç«‹|è‡ªç”±|æŸç¼š|æ§åˆ¶|å æœ‰|ç‰ºç‰²|ä»˜å‡º|å›æŠ¥|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|ç—›è‹¦|å¿«ä¹)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
            r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(çš„)?(ç”·å‹|å¥³æœ‹å‹|è€å…¬|è€å©†|å¦»å­|ä¸ˆå¤«|ä¼´ä¾£|æ‹äºº|çˆ±äºº|å¯¹è±¡|å‰ä»»|å‰ç”·å‹|å‰å¥³å‹|å‰å¤«|å‰å¦»|çˆ¶æ¯|çˆ¸çˆ¸|å¦ˆå¦ˆ|çˆ¶äº²|æ¯äº²|å­©å­|å„¿å­|å¥³å„¿|å…„å¼Ÿ|å§å¦¹|å“¥å“¥|å¼Ÿå¼Ÿ|å§å§|å¦¹å¦¹|æœ‹å‹|é—ºèœœ|å“¥ä»¬|åŒäº‹|åŒå­¦|è€å¸ˆ|å­¦ç”Ÿ|é¢†å¯¼|ä¸‹å±)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(ä¸ç†|ä¸è”ç³»|ä¸å›å¤|ä¸å›æ¶ˆæ¯|ä¸æ¥ç”µè¯|ä¸è§é¢|ä¸å…³å¿ƒ|ä¸åœ¨ä¹|ä¸é‡è§†|ä¸ç†è§£|ä¸æ”¯æŒ|ä¸ä¿¡ä»»|ä¸å°Šé‡|ä¸åŒ…å®¹|ä¸ä½“è´´|ä¸çˆ±|ä¸å–œæ¬¢|è®¨åŒ|æ¨|ä¼¤å®³|æ¬ºéª—|èƒŒå›|å‡ºè½¨|åŠˆè…¿|è¯´è°|éšç’|è¯¯ä¼š|å†¤æ‰|æŒ‡è´£|æ‰¹è¯„|æŠ±æ€¨|åŸ‹æ€¨|è´£æ€ª|äº‰åµ|åµæ¶|æ‰“æ¶|å†·æˆ˜|åˆ†æ‰‹|ç¦»å©š|ç¦»å¼€|æŠ›å¼ƒ|æ”¾å¼ƒ|ç–è¿œ|å†·æ·¡|å¿½è§†|èº²é¿|é€ƒé¿|æ‹’ç»|æ’æ–¥|å«‰å¦’|ç¾¡æ…•|æ§åˆ¶|å æœ‰|æŸç¼š|å‹åŠ›|çƒ¦æ¼|å›°æ‰°|ç—›è‹¦|ä¼¤å¿ƒ|éš¾è¿‡|å¤±æœ›|ç»æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]"
        ]

        # é‡‡æ ·æ•°æ®é›†çš„å‰sample_sizeè¡Œ
        with open(self.new_data_path, 'r', encoding='utf-8') as f:
            line_count = 0
            for line in tqdm(f, desc="éªŒè¯æ‰“ä¹±è´¨é‡", total=self.shuffle_sample_size):
                if line_count >= self.shuffle_sample_size:
                    break

                try:
                    data = json.loads(line)
                    text = data.get('text', '').lower()

                    # è¯†åˆ«ä¸»é¢˜ - ä¼˜åŒ–é€»è¾‘
                    topic = "general"

                    # 1. æ£€æŸ¥æ˜¯å¦æƒ…æ„Ÿå¯¹è¯ (æ›´ä¸¥æ ¼çš„æ¡ä»¶)
                    is_emotional = False
                    for pattern in emotional_patterns:
                        if re.search(pattern, text):
                            is_emotional = True
                            break

                    # 2. æ£€æŸ¥å…¶ä»–ä¸»é¢˜
                    found_other_topic = False
                    for topic_name, keywords in topic_keywords.items():
                        # è·³è¿‡æƒ…æ„Ÿä¸»é¢˜ï¼Œå•ç‹¬å¤„ç†
                        if topic_name == "emotional":
                            continue

                        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¥ä¸»é¢˜çš„å…³é”®è¯
                        if any(keyword in text for keyword in keywords):
                            topic = topic_name
                            found_other_topic = True
                            break

                    # 3. å¦‚æœæ²¡æœ‰å…¶ä»–ä¸»é¢˜ä½†ç¬¦åˆæƒ…æ„Ÿå¯¹è¯æ¡ä»¶
                    if not found_other_topic and is_emotional:
                        topic = "emotional"

                    topic_sequence.append(topic)
                    topic_count[topic] += 1
                    line_count += 1
                except:
                    continue

        # è®¡ç®—åŒä¸»é¢˜è¿ç»­å‡ºç°æ¬¡æ•°
        max_streak = 0
        current_streak = 0
        current_topic = None

        for topic in topic_sequence:
            if topic == current_topic:
                current_streak += 1
                max_streak = max(max_streak, current_streak)
            else:
                current_streak = 1
                current_topic = topic

        # è®¡ç®—ä¸»é¢˜è½¬æ¢é¢‘ç‡
        topic_changes = 0
        if len(topic_sequence) > 1:
            topic_changes = sum(1 for i in range(1, len(topic_sequence))
                                if topic_sequence[i] != topic_sequence[i - 1])
        change_rate = topic_changes / len(topic_sequence) if topic_sequence else 0

        # è®¡ç®—ä¸»é¢˜åˆ†å¸ƒç†µï¼ˆè¡¡é‡å¤šæ ·æ€§ï¼‰
        total = len(topic_sequence)
        probs = [count / total for count in topic_count.values()] if total > 0 else []
        entropy_val = -sum(p * math.log(p + 1e-10) for p in probs) if probs else 0

        # æƒ…æ„Ÿå¯¹è¯åˆ†å¸ƒæŠ¥å‘Š
        emotional_percent = topic_count.get("emotional", 0) / total * 100 if total > 0 else 0

        # è®¡ç®—æ‰€æœ‰ä¸»é¢˜å æ¯”
        topic_percentages = {}
        for topic, count in topic_count.items():
            topic_percentages[topic] = count / total * 100

        # è®°å½•ç»“æœ
        self.results['shuffle_quality'] = {
            'max_streak': max_streak,
            'topic_changes': topic_changes,
            'change_rate': change_rate,
            'entropy': entropy_val,
            'emotional_percent': emotional_percent,
            'topic_distribution': dict(topic_count),
            'topic_percentages': topic_percentages,  # æ–°å¢ä¸»é¢˜å æ¯”å­—å…¸
            'line_count': total
        }

        print(f"  - æœ€å¤§è¿ç»­ç›¸åŒä¸»é¢˜: {max_streak}è¡Œ")
        print(f"  - ä¸»é¢˜è½¬æ¢é¢‘ç‡: {change_rate:.4f} (æ¯è¡Œ)")
        print(f"  - ä¸»é¢˜åˆ†å¸ƒç†µ: {entropy_val:.4f}")
        print(f"  - æƒ…æ„Ÿå¯¹è¯å æ¯”: {emotional_percent:.2f}%")

        # è¾“å‡ºæ‰€æœ‰ä¸»é¢˜å æ¯”
        print("\n  === æ‰€æœ‰ä¸»é¢˜å æ¯” ===")
        for topic, percentage in sorted(topic_percentages.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {topic}: {percentage:.2f}%")

        print("  ===================")

        # è¯„ä¼°æ ‡å‡†ï¼ˆå¢åŠ æƒ…æ„Ÿå¯¹è¯æƒé‡ï¼‰
        quality_score = 0

        # 1. æœ€å¤§è¿ç»­ä¸»é¢˜è¡Œæ•°
        if max_streak <= 10:
            quality_score += 2
        elif max_streak <= 20:
            quality_score += 1

        # 2. ä¸»é¢˜è½¬æ¢é¢‘ç‡
        if change_rate >= 0.5:
            quality_score += 2
        elif change_rate >= 0.3:
            quality_score += 1

        # 3. ä¸»é¢˜åˆ†å¸ƒç†µ
        if entropy_val >= 1.5:
            quality_score += 1
        elif entropy_val >= 1.0:
            quality_score += 0.5

        # 4. æƒ…æ„Ÿå¯¹è¯åˆ†å¸ƒ
        if 5 <= emotional_percent <= 20:  # ç†æƒ³å æ¯”èŒƒå›´
            quality_score += 1
        elif emotional_percent > 0:  # è‡³å°‘æœ‰ä¸€å®šæ¯”ä¾‹
            quality_score += 0.5

        # è¾“å‡ºè´¨é‡æŠ¥å‘Š
        quality_status = ""
        if quality_score >= 5:
            quality_status = "ä¼˜ç§€"
            print("âœ… æ•°æ®é›†å·²å……åˆ†æ‰“ä¹± (è´¨é‡è¯„åˆ†: ä¼˜ç§€)")
        elif quality_score >= 4:
            quality_status = "è‰¯å¥½"
            print("âš ï¸ æ•°æ®é›†æ‰“ä¹±åŸºæœ¬åˆæ ¼ (è´¨é‡è¯„åˆ†: è‰¯å¥½)")
        elif quality_score >= 3:
            quality_status = "ä¸­ç­‰"
            print("âš ï¸ æ•°æ®é›†æ‰“ä¹±å‹‰å¼ºåˆæ ¼ (è´¨é‡è¯„åˆ†: ä¸­ç­‰)")
        else:
            quality_status = "å·®"
            print("âŒ æ•°æ®é›†æ‰“ä¹±ä¸å……åˆ† (è´¨é‡è¯„åˆ†: å·®)")

        # è®°å½•è´¨é‡è¯„åˆ†
        self.results['shuffle_quality']['quality_score'] = quality_score
        self.results['shuffle_quality']['quality_status'] = quality_status

        return quality_status in ["ä¼˜ç§€", "è‰¯å¥½", "ä¸­ç­‰"]

    # def load_sample_data(self):
    #     """ä»æ–°æ•°æ®é›†åŠ è½½é‡‡æ ·æ•°æ®"""
    #     print(f"â–¶ ä» {self.new_data_path} åŠ è½½æ•°æ®æ ·æœ¬...")
    #     sample_text = ""
    #     total_chars = 0
    #
    #     if os.path.isdir(self.new_data_path):
    #         files = [f for f in os.listdir(self.new_data_path) if f.endswith(('.txt', '.json', '.jsonl'))]
    #     else:
    #         files = [self.new_data_path]
    #
    #     for file_path in files:
    #         full_path = file_path if os.path.isfile(file_path) else os.path.join(self.new_data_path, file_path)
    #
    #         try:
    #             if file_path.endswith('.jsonl'):
    #                 with open(full_path, 'r', encoding='utf-8') as f:
    #                     for line in tqdm(f, desc=f"å¤„ç† {file_path}"):
    #                         if total_chars >= self.sample_size:
    #                             break
    #                         data = json.loads(line)
    #                         text = data.get('text', '') if isinstance(data, dict) else str(data)
    #                         sample_text += text + " "
    #                         total_chars += len(text)
    #
    #             else:  # txt æˆ–å…¶ä»–æ–‡æœ¬æ–‡ä»¶
    #                 with open(full_path, 'r', encoding='utf-8') as f:
    #                     for line in tqdm(f, desc=f"å¤„ç† {file_path}"):
    #                         if total_chars >= self.sample_size:
    #                             break
    #                         sample_text += line
    #                         total_chars += len(line)
    #
    #             if total_chars >= self.sample_size:
    #                 break
    #
    #         except Exception as e:
    #             print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
    #
    #     # ç¡®ä¿æ ·æœ¬ä¸è¶…è¿‡æŒ‡å®šå¤§å°
    #     self.sample_text = sample_text[:self.sample_size]
    #     print(f"âœ… å·²åŠ è½½ {len(self.sample_text)} å­—ç¬¦çš„æ ·æœ¬æ•°æ®")
    #     return self.sample_text
    def load_sample_data(self):
        """ä»æ–°æ•°æ®é›†éšæœºæŠ½æ ·é‡‡æ ·æ•°æ®"""
        print(f"â–¶ ä» {self.new_data_path} éšæœºæŠ½æ ·åŠ è½½æ•°æ®æ ·æœ¬...")

        sample_text = ""
        total_chars = 0

        def get_file_line_count(filepath):
            count = 0
            with open(filepath, 'r', encoding='utf-8') as f:
                for _ in f:
                    count += 1
            return count

        if os.path.isdir(self.new_data_path):
            files = [os.path.join(self.new_data_path, f) for f in os.listdir(self.new_data_path)
                     if f.endswith(('.txt', '.json', '.jsonl'))]
        else:
            files = [self.new_data_path]

        # å¯¹æ¯ä¸ªæ–‡ä»¶å•ç‹¬éšæœºæŠ½æ ·
        for file_path in files:
            ext = os.path.splitext(file_path)[1].lower()

            if ext == '.jsonl':
                # å…ˆç»Ÿè®¡è¡Œæ•°
                line_count = get_file_line_count(file_path)
                if line_count == 0:
                    continue

                # æŒ‰è¡Œéšæœºé‡‡æ ·ç´¢å¼• (è¿™é‡Œç®€å•é‡‡æ ·1ä¸‡è¡Œæˆ–æ›´å¤šï¼Œæ ¹æ®sample_sizeè°ƒæ•´)
                sample_line_num = min(10000, line_count)
                sampled_lines_idx = set(random.sample(range(line_count), sample_line_num))

                with open(file_path, 'r', encoding='utf-8') as f:
                    for i, line in enumerate(f):
                        if i in sampled_lines_idx:
                            try:
                                data = json.loads(line)
                                text = data.get('text', '') if isinstance(data, dict) else str(data)
                                sample_text += text + " "
                                total_chars += len(text)
                            except:
                                continue

                            if total_chars >= self.sample_size:
                                break
            else:
                # æ™®é€šæ–‡æœ¬æ–‡ä»¶ï¼Œå…ˆè¯»æ‰€æœ‰è¡Œï¼ŒéšæœºæŠ½
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                line_count = len(lines)
                if line_count == 0:
                    continue

                sample_line_num = min(10000, line_count)
                sampled_lines_idx = set(random.sample(range(line_count), sample_line_num))

                for i in sampled_lines_idx:
                    sample_text += lines[i]
                    total_chars += len(lines[i])
                    if total_chars >= self.sample_size:
                        break

            if total_chars >= self.sample_size:
                break

        # ç¡®ä¿æ ·æœ¬ä¸è¶…è¿‡æŒ‡å®šå¤§å°
        self.sample_text = sample_text[:self.sample_size]
        print(f"âœ… å·²éšæœºæŠ½æ ·åŠ è½½ {len(self.sample_text)} å­—ç¬¦çš„æ ·æœ¬æ•°æ®")
        return self.sample_text

    def calculate_oov_rate(self):
        """è®¡ç®—æœªç™»å½•è¯ç‡ (OOV Rate)"""
        print("â–¶ è®¡ç®—æœªç™»å½•è¯ç‡...")
        tokens = self.tokenizer.tokenize(self.sample_text)
        token_count = len(tokens)

        if token_count == 0:
            return 0.0

        oov_count = sum(1 for t in tokens if t == self.unk_token)
        oov_rate = oov_count / token_count

        # è®°å½•ç»“æœ
        self.results['oov_rate'] = oov_rate
        self.results['token_count'] = token_count
        self.results['oov_count'] = oov_count

        print(f"  - æ€»tokenæ•°: {token_count}")
        print(f"  - æœªç™»å½•tokenæ•°: {oov_count}")
        print(f"  - OOVç‡: {oov_rate:.4f} ({oov_rate * 100:.2f}%)")
        return oov_rate

    def analyze_vocab_distribution(self):
        """åˆ†ææ–°æ—§æ•°æ®é›†çš„è¯é¢‘åˆ†å¸ƒ"""
        print("â–¶ åˆ†æè¯æ±‡åˆ†å¸ƒ...")

        # æ–°æ•°æ®é›†è¯é¢‘
        new_tokens = self.tokenizer.tokenize(self.sample_text)
        new_token_freq = Counter(new_tokens)
        total_new_tokens = sum(new_token_freq.values())

        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        for st in self.special_tokens:
            new_token_freq.pop(st, None)

        # è·å–åŸå§‹åˆ†è¯å™¨çš„è¯æ±‡é¢‘ç‡ (å¦‚æœå¯ç”¨)
        original_freq = {}
        if hasattr(self.tokenizer, 'vocab') and hasattr(self.tokenizer, 'get_vocab'):
            # å‡è®¾å‡åŒ€åˆ†å¸ƒä½œä¸ºåŸºçº¿
            vocab_size = len(self.tokenizer.get_vocab())
            original_freq = {token: 1 / vocab_size for token in self.original_vocab - self.special_tokens}

        # è®¡ç®—KLæ•£åº¦
        kl_div = 0.0
        all_tokens = set(original_freq.keys()) | set(new_token_freq.keys())

        # ä¸ºKLè®¡ç®—åˆ›å»ºæ¦‚ç‡åˆ†å¸ƒ
        P = []
        Q = []
        for token in all_tokens:
            p_val = original_freq.get(token, 1e-10)  # åŸå§‹æ¦‚ç‡
            q_val = new_token_freq.get(token, 0) / total_new_tokens  # æ–°æ•°æ®é›†æ¦‚ç‡

            # é¿å…é›¶æ¦‚ç‡
            if q_val == 0:
                q_val = 1e-10

            P.append(p_val)
            Q.append(q_val)

        # è®¡ç®—KLæ•£åº¦: D_KL(P || Q)
        kl_div = entropy(P, Q)

        # è®°å½•ç»“æœ
        self.results['kl_divergence'] = kl_div
        self.results['new_vocab_size'] = len(new_token_freq)
        self.results['original_vocab_size'] = len(self.original_vocab)

        print(f"  - åŸå§‹è¯æ±‡å¤§å°: {len(self.original_vocab)}")
        print(f"  - æ–°æ•°æ®é›†æœ‰æ•ˆè¯æ±‡: {len(new_token_freq)}")
        print(f"  - KLæ•£åº¦: {kl_div:.4f}")
        return kl_div

    def analyze_token_coverage(self, top_n=50):
        """åˆ†ætokenè¦†ç›–æƒ…å†µ"""
        print("â–¶ åˆ†ætokenè¦†ç›–...")

        # è·å–æ–°æ•°æ®é›†ä¸­çš„token
        new_tokens = set(self.tokenizer.tokenize(self.sample_text))

        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        new_tokens = new_tokens - self.special_tokens
        original_vocab = self.original_vocab - self.special_tokens

        # è®¡ç®—è¦†ç›–æ¯”ä¾‹
        covered = new_tokens & original_vocab
        uncovered = new_tokens - original_vocab

        coverage_ratio = len(covered) / len(new_tokens) if new_tokens else 0

        # è®°å½•ç»“æœ
        self.results['coverage_ratio'] = coverage_ratio
        self.results['covered_tokens'] = len(covered)
        self.results['uncovered_tokens'] = len(uncovered)

        print(f"  - è¦†ç›–æ¯”ä¾‹: {coverage_ratio:.4f}")
        print(f"  - è¦†ç›–tokenæ•°: {len(covered)}")
        print(f"  - æœªè¦†ç›–tokenæ•°: {len(uncovered)}")

        # è·å–æœ€å¸¸è§çš„æœªè¦†ç›–token
        all_tokens = self.tokenizer.tokenize(self.sample_text)
        token_counter = Counter(all_tokens)

        # è¿‡æ»¤æ‰å·²è¦†ç›–å’Œç‰¹æ®Štoken
        uncovered_counter = {t: c for t, c in token_counter.items()
                             if t in uncovered and t not in self.special_tokens}

        # å–å‰Nä¸ªæœ€å¸¸è§çš„æœªè¦†ç›–token
        top_uncovered = sorted(uncovered_counter.items(), key=lambda x: x[1], reverse=True)[:top_n]
        self.results['top_uncovered'] = top_uncovered

        return coverage_ratio, top_uncovered

    def visualize_results(self):
        """åˆ›å»ºå¯è§†åŒ–æŠ¥å‘Š"""
        if not self.results:
            print("âš ï¸ è¯·å…ˆè¿è¡Œåˆ†æ")
            return

        print("â–¶ ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
        fig = plt.figure(figsize=(15, 12))

        # 1. æ‰“ä¹±è´¨é‡å±•ç¤º (å·¦ä¾§)
        ax1 = fig.add_subplot(2, 2, 1)
        if 'shuffle_quality' in self.results:
            sq = self.results['shuffle_quality']

            # åˆ›å»ºä¸»é¢˜å æ¯”é¥¼å›¾
            topic_percentages = sq['topic_percentages']

            # è¿‡æ»¤æ‰å æ¯”è¿‡å°çš„ä¸»é¢˜
            filtered_topics = {k: v for k, v in topic_percentages.items() if v >= 1.0}
            other_percent = 100 - sum(filtered_topics.values())

            if other_percent > 0:
                filtered_topics['å…¶ä»–'] = other_percent

            labels = list(filtered_topics.keys())
            sizes = list(filtered_topics.values())

            # åˆ›å»ºé¢œè‰²æ˜ å°„
            colors = plt.cm.tab20c(np.linspace(0, 1, len(labels)))

            # ç»˜åˆ¶é¥¼å›¾
            wedges, texts, autotexts = ax1.pie(
                sizes,
                labels=labels,
                autopct='%1.1f%%',
                startangle=90,
                colors=colors,
                textprops={'fontsize': 9}
            )

            # æ·»åŠ æ ‡é¢˜
            ax1.set_title('ä¸»é¢˜åˆ†å¸ƒå æ¯”')
            ax1.axis('equal')  # ç­‰æ¯”ä¾‹ç¡®ä¿é¥¼å›¾æ˜¯åœ†å½¢

            # æ·»åŠ å›¾ä¾‹
            ax1.legend(
                wedges,
                labels,
                title="ä¸»é¢˜",
                loc="center left",
                bbox_to_anchor=(1, 0, 0.5, 1),
                fontsize=9
            )
        else:
            ax1.text(0.5, 0.5, 'æœªè¿›è¡Œæ‰“ä¹±éªŒè¯', ha='center', va='center')
            ax1.set_title('ä¸»é¢˜åˆ†å¸ƒå æ¯”')

        # 2. åˆ†è¯å™¨é€‚é…æ€§æŒ‡æ ‡ (å³ä¸Š)
        ax2 = fig.add_subplot(2, 2, 2)
        oov_rate = self.results['oov_rate']
        coverage = self.results.get('coverage_ratio', 0)
        kl_div = self.results['kl_divergence']

        metrics = ['OOVç‡', 'è¦†ç›–æ¯”ä¾‹', 'KLæ•£åº¦']
        values = [oov_rate, coverage, kl_div]

        # åˆ›å»ºé¢œè‰²æ˜ å°„
        colors = []
        for val, metric in zip(values, metrics):
            if metric == 'OOVç‡':
                colors.append('red' if val > 0.05 else 'orange' if val > 0.03 else 'green')
            elif metric == 'è¦†ç›–æ¯”ä¾‹':
                colors.append('green' if val > 0.95 else 'orange' if val > 0.90 else 'red')
            else:  # KLæ•£åº¦
                colors.append('red' if val > 2.0 else 'orange' if val > 1.0 else 'green')

        bars = ax2.bar(metrics, values, color=colors)
        ax2.set_title('åˆ†è¯å™¨é€‚é…æ€§æŒ‡æ ‡')
        ax2.set_ylabel('æŒ‡æ ‡å€¼')

        # æ·»åŠ é˜ˆå€¼çº¿
        ax2.axhline(y=0.03, color='green', linestyle='--', alpha=0.5)
        ax2.axhline(y=0.05, color='orange', linestyle='--', alpha=0.5)
        ax2.axhline(y=1.0, color='orange', linestyle='--', alpha=0.5)
        ax2.axhline(y=2.0, color='red', linestyle='--', alpha=0.5)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            ax2.annotate(f'{height:.3f}',
                         xy=(bar.get_x() + bar.get_width() / 2, height),
                         xytext=(0, 3),
                         textcoords="offset points",
                         ha='center', va='bottom')

        # 3. æ‰“ä¹±è´¨é‡æŒ‡æ ‡ (å·¦ä¸‹)
        ax3 = fig.add_subplot(2, 2, 3)
        if 'shuffle_quality' in self.results:
            sq = self.results['shuffle_quality']

            # åˆ›å»ºè´¨é‡æŒ‡æ ‡æ¡å½¢å›¾
            metrics = ['æœ€å¤§è¿ç»­ä¸»é¢˜', 'ä¸»é¢˜è½¬æ¢é¢‘ç‡', 'ä¸»é¢˜åˆ†å¸ƒç†µ', 'æƒ…æ„Ÿå¯¹è¯å æ¯”']
            values = [
                sq['max_streak'],
                sq['change_rate'],
                sq['entropy'],
                sq['emotional_percent'] / 100  # è½¬æ¢ä¸ºæ¯”ä¾‹
            ]

            # åˆ›å»ºé¢œè‰²æ˜ å°„ï¼ˆå€¼è¶Šä½è¶Šçº¢ï¼Œè¶Šé«˜è¶Šç»¿ï¼‰
            colors = []
            for val in values:
                if val < 0.3:  # ä½å€¼èŒƒå›´
                    colors.append(plt.cm.Reds(0.3 + val * 0.7))
                else:  # é«˜å€¼èŒƒå›´
                    colors.append(plt.cm.Greens(val))

            bars = ax3.bar(metrics, values, color=colors)
            ax3.set_title('æ‰“ä¹±è´¨é‡æŒ‡æ ‡')
            ax3.set_ylabel('æŒ‡æ ‡å€¼')

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar in bars:
                height = bar.get_height()
                ax3.annotate(f'{height:.3f}',
                             xy=(bar.get_x() + bar.get_width() / 2, height),
                             xytext=(0, 3),  # 3 points vertical offset
                             textcoords="offset points",
                             ha='center', va='bottom')
        else:
            ax3.text(0.5, 0.5, 'æœªè¿›è¡Œæ‰“ä¹±éªŒè¯', ha='center', va='center')
            ax3.set_title('æ‰“ä¹±è´¨é‡æŒ‡æ ‡')

        # 4. æœªè¦†ç›–tokenå±•ç¤º (å³ä¸‹)
        ax4 = fig.add_subplot(2, 2, 4)
        top_uncovered = self.results.get('top_uncovered', [])

        if top_uncovered:
            tokens, counts = zip(*top_uncovered)
            y_pos = np.arange(len(tokens))

            ax4.barh(y_pos, counts, color='salmon')
            ax4.set_yticks(y_pos)
            ax4.set_yticklabels(tokens, fontsize=8)
            ax4.set_xlabel('å‡ºç°é¢‘ç‡')
            ax4.set_title('Top æœªè¦†ç›–Token')
            ax4.invert_yaxis()  # æœ€é«˜é¢‘ç‡åœ¨é¡¶éƒ¨
        else:
            ax4.text(0.5, 0.5, 'æ— æ˜¾è‘—æœªè¦†ç›–token', ha='center', va='center')
            ax4.set_title('æœªè¦†ç›–Tokenåˆ†æ')

        plt.tight_layout()

        # ä¿å­˜æŠ¥å‘Š
        report_path = "tokenizer_evaluation_report.png"
        plt.savefig(report_path)
        print(f"âœ… å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜è‡³ {report_path}")

        return report_path

    def generate_recommendation(self):
        """ç”Ÿæˆå¤„ç†å»ºè®®"""
        if not self.results:
            print("âš ï¸ è¯·å…ˆè¿è¡Œåˆ†æ")
            return

        oov_rate = self.results['oov_rate']
        kl_div = self.results['kl_divergence']
        coverage = self.results.get('coverage_ratio', 0)

        print("\n" + "=" * 50)
        print("ğŸ” åˆ†è¯å™¨é€‚é…è¯„ä¼°æŠ¥å‘Š")
        print("=" * 50)
        print(f"  - æœªç™»å½•è¯ç‡ (OOV): {oov_rate * 100:.2f}%")
        print(f"  - è¯é¢‘åˆ†å¸ƒå·®å¼‚ (KLæ•£åº¦): {kl_div:.2f}")
        print(f"  - Tokenè¦†ç›–æ¯”ä¾‹: {coverage * 100:.2f}%")

        # å¦‚æœè¿›è¡Œäº†æ‰“ä¹±éªŒè¯ï¼Œå±•ç¤ºç›¸å…³ä¿¡æ¯
        if 'shuffle_quality' in self.results:
            sq = self.results['shuffle_quality']
            print(f"  - æ‰“ä¹±è´¨é‡è¯„åˆ†: {sq.get('quality_score', 'N/A')} ({sq.get('quality_status', 'æœªçŸ¥')})")
            print(f"  - æƒ…æ„Ÿå¯¹è¯å æ¯”: {sq.get('emotional_percent', 0):.2f}%")
            print(f"  - æœ€å¤§è¿ç»­ä¸»é¢˜: {sq.get('max_streak', 0)}è¡Œ")

            # è¾“å‡ºæ‰€æœ‰ä¸»é¢˜å æ¯”
            print("\n  === æ‰€æœ‰ä¸»é¢˜å æ¯” ===")
            for topic, percentage in sorted(sq['topic_percentages'].items(), key=lambda x: x[1], reverse=True):
                print(f"  - {topic}: {percentage:.2f}%")
            print("  ===================")

        # å†³ç­–é€»è¾‘
        recommendation = ""
        action = ""

        if oov_rate < 0.03 and kl_div < 1.0:
            recommendation = "âœ… å¯ç›´æ¥å¤ç”¨ç°æœ‰åˆ†è¯å™¨"
            action = "æ— éœ€ä»»ä½•æ“ä½œ"
        elif oov_rate < 0.10 and kl_div < 2.0:
            recommendation = "âš ï¸ å»ºè®®æ‰©å±•è¯æ±‡è¡¨"
            action = "ä½¿ç”¨æ–°æ•°æ®é›†æ‰©å±•ç°æœ‰è¯æ±‡è¡¨"
        else:
            recommendation = "âŒ éœ€è¦é‡æ–°è®­ç»ƒåˆ†è¯å™¨"
            action = "åŸºäºæ–°æ—§æ•°æ®é›†è”åˆè®­ç»ƒæ–°åˆ†è¯å™¨"

        # æ·»åŠ è¯¦ç»†è§£é‡Š
        print("\nğŸ’¡ å»ºè®®:")
        print(f"  - {recommendation}")
        print(f"  - æ“ä½œ: {action}")

        if oov_rate > 0.05:
            print(f"    (OOVç‡è¶…è¿‡5%å®‰å…¨é˜ˆå€¼)")
        if kl_div > 1.5:
            print(f"    (è¯é¢‘åˆ†å¸ƒå·®å¼‚è¾ƒå¤§ï¼ŒKLæ•£åº¦={kl_div:.2f})")

        # æ·»åŠ é¢å¤–å»ºè®®
        if "æ‰©å±•" in recommendation and self.results.get('top_uncovered'):
            print("\nğŸ”§ æ‰©å±•è¯æ±‡è¡¨å»ºè®®:")
            print("   - æ·»åŠ ä»¥ä¸‹é«˜é¢‘æœªè¦†ç›–token:")
            for i, (token, count) in enumerate(self.results['top_uncovered'][:10]):
                print(f"     {i + 1}. {token} (å‡ºç°æ¬¡æ•°: {count})")

        if "é‡æ–°è®­ç»ƒ" in recommendation:
            print("\nğŸ”§ é‡æ–°è®­ç»ƒå»ºè®®:")
            print("   - ä½¿ç”¨è”åˆè®­ç»ƒ: åˆå¹¶æ–°æ—§æ•°æ®é›†")
            print("   - è€ƒè™‘è°ƒæ•´è¯æ±‡è¡¨å¤§å°")
            print("   - è¯„ä¼°ä¸åŒåˆ†è¯ç®—æ³• (BPE/WordPiece/Unigram)")

        # æ‰“ä¹±è´¨é‡å»ºè®®
        if 'shuffle_quality' in self.results:
            sq = self.results['shuffle_quality']
            if sq.get('quality_status') in ['ä¸­ç­‰', 'å·®']:
                print("\nğŸ”§ æ•°æ®é›†æ‰“ä¹±å»ºè®®:")
                print(f"   - æƒ…æ„Ÿå¯¹è¯å æ¯”åº”æ§åˆ¶åœ¨5-20%ä¹‹é—´ (å½“å‰: {sq.get('emotional_percent', 0):.2f}%)")
                if sq.get('max_streak', 0) > 20:
                    print(f"   - å‡å°‘è¿ç»­ç›¸åŒä¸»é¢˜è¡Œæ•° (å½“å‰æœ€å¤§è¿ç»­: {sq.get('max_streak', 0)}è¡Œ)")
                if sq.get('change_rate', 0) < 0.3:
                    print(f"   - æé«˜ä¸»é¢˜è½¬æ¢é¢‘ç‡ (å½“å‰: {sq.get('change_rate', 0):.2f})")
                # ä¸»é¢˜åˆ†å¸ƒå»ºè®®
                print("   - ä¸»é¢˜åˆ†å¸ƒä¼˜åŒ–:")
                for topic, percent in sq['topic_percentages'].items():
                    if percent < 5.0 and topic != "emotional":
                        print(f"     - å¢åŠ  '{topic}' ä¸»é¢˜å†…å®¹ (å½“å‰: {percent:.2f}%)")
                    elif percent > 30.0:
                        print(f"     - å‡å°‘ '{topic}' ä¸»é¢˜å†…å®¹ (å½“å‰: {percent:.2f}%)")

        print("=" * 50 + "\n")

        return recommendation, action

    def run_full_analysis(self):
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        try:
            # å¦‚æœæ˜¯JSONLæ–‡ä»¶ï¼Œå…ˆéªŒè¯æ‰“ä¹±è´¨é‡
            if self.new_data_path.endswith('.jsonl'):
                self.validate_shuffle()

            self.load_sample_data()
            self.calculate_oov_rate()
            self.analyze_vocab_distribution()
            self.analyze_token_coverage()
            report_path = self.visualize_results()
            recommendation = self.generate_recommendation()

            # ä¿å­˜ç»“æœåˆ°JSON
            with open("tokenizer_eval_results.json", "w") as f:
                json.dump(self.results, f, indent=2)

            return {
                "status": "success",
                "report_image": report_path,
                "results": self.results,
                "recommendation": recommendation
            }
        except Exception as e:
            return {
                "status": "error",
                "message": str(e)
            }


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    evaluator = TokenizerEvaluator(
        tokenizer_path="D:\\PythonCode\\RainLLM\\models\\new_tokenizer",
        new_data_path="D:\PythonCode\RainLLM\dataset\sft_512.jsonl",
        sample_size=500000,  # 50ä¸‡å­—ç¬¦æ ·æœ¬
        shuffle_sample_size=10000  # 1ä¸‡è¡Œæ‰“ä¹±éªŒè¯
    )

    results = evaluator.run_full_analysis()
    print("\nè¯„ä¼°å®Œæˆï¼ç»“æœæ‘˜è¦:")
    print(results.get("recommendation", {}))