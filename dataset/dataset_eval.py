# import json
# import re
# import os
# import time
# import math
# import random
# from collections import defaultdict
# from tqdm import tqdm
#
#
# class DataSetEvaluator:
#     def __init__(self, data_path, sample_size=1000000, shuffle_sample_size=10000):
#         self.data_path = data_path
#         self.sample_size = sample_size
#         self.shuffle_sample_size = shuffle_sample_size
#         self.results = {}
#         self.file_size = os.path.getsize(data_path) if os.path.exists(data_path) else 0
#
#         # ä¸»é¢˜å…³é”®è¯å®šä¹‰
#         self.topic_keywords = {
#             "medical": ["åŒ»å­¦", "åŒ»ç–—", "å¥åº·", "ç–¾ç—…", "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‚£è€…", "æ²»ç–—", "è¯Šæ–­", "è¯ç‰©", "æ‰‹æœ¯"],
#             "code": ["ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "å˜é‡", "ç±»", "æ–¹æ³•", "ç®—æ³•", "æ•°æ®åº“", "API", "ç¼–ç¨‹è¯­è¨€", "è°ƒè¯•", "æ¡†æ¶"],
#             "finance": ["é‡‘è", "è‚¡ç¥¨", "æŠ•èµ„", "é“¶è¡Œ", "ç»æµ", "å¸‚åœº", "äº¤æ˜“", "è´§å¸", "æ±‡ç‡", "åŸºé‡‘", "ç†è´¢"],
#             "education": ["æ•™è‚²", "å­¦æ ¡", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™å­¦", "è€å¸ˆ", "å­¦ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "æ•™æ", "å­¦ä½"],
#             "tech": ["æŠ€æœ¯", "ç§‘æŠ€", "åˆ›æ–°", "ç ”å‘", "å·¥ç¨‹", "è®¾å¤‡", "ç³»ç»Ÿ", "åº”ç”¨", "æ™ºèƒ½", "æœºå™¨äºº", "äººå·¥æ™ºèƒ½"],
#             "entertainment": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ¸¸æˆ", "æ˜æ˜Ÿ", "èŠ‚ç›®", "æ¼”å‡º", "ç»¼è‰º", "è‰ºäºº", "æ¼”å”±ä¼š",
#                               "ç”µè§†å‰§"],
#             "emotional": ["æƒ…æ„Ÿ", "å¿ƒæƒ…", "æ„Ÿå—", "çˆ±", "å–œæ¬¢", "æ‹çˆ±", "åˆ†æ‰‹", "å®‰æ…°", "æ”¯æŒ", "ç†è§£", "å…³ç³»",
#                           "æœ‹å‹", "å®¶äºº", "å­¤ç‹¬", "å¼€å¿ƒ", "ä¼¤å¿ƒ", "æ„¤æ€’", "å‹åŠ›", "å¿ƒç†å’¨è¯¢", "æƒ…ç»ªç®¡ç†", "å…±æƒ…",
#                           "äº²å¯†å…³ç³»", "æƒ…æ„Ÿè¡¨è¾¾", "å¿ƒç†æ”¯æŒ", "æƒ…æ„Ÿéœ€æ±‚", "æƒ…æ„Ÿå›°æƒ‘", "æƒ…æ„Ÿäº¤æµ", "æƒ…æ„Ÿå’¨è¯¢"],
#             "science": ["ç§‘å­¦", "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©", "æ•°å­¦", "å¤©æ–‡", "åœ°ç†", "å®éªŒ", "ç†è®º", "ç ”ç©¶", "è‡ªç„¶"],
#             "literature": ["æ–‡å­¦", "å°è¯´", "è¯—æ­Œ", "æ•£æ–‡", "ä½œå®¶", "ä½œå“", "é˜…è¯»", "å†™ä½œ", "æ•…äº‹", "æƒ…èŠ‚", "äººç‰©"],
#             "sports": ["ä½“è‚²", "è¿åŠ¨", "æ¯”èµ›", "è¶³çƒ", "ç¯®çƒ", "è¿åŠ¨å‘˜", "è®­ç»ƒ", "å¥¥è¿", "å† å†›", "èµ›äº‹", "å¥èº«"]
#         }
#
#         # æƒ…æ„Ÿæ¨¡å¼æ­£åˆ™è¡¨è¾¾å¼
#         self.emotional_patterns = [
#             r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(æ„Ÿåˆ°|è§‰å¾—|æ„Ÿè§‰|è®¤ä¸º|ä»¥ä¸º|å‘ç°|çŸ¥é“|å¸Œæœ›|æƒ³è¦|éœ€è¦|å–œæ¬¢|çˆ±|è®¨åŒ|æ¨|å®³æ€•|æ‹…å¿ƒ|ç„¦è™‘|ç”Ÿæ°”|æ„¤æ€’|ä¼¤å¿ƒ|éš¾è¿‡|å¼€å¿ƒ|å¿«ä¹|å…´å¥‹|æƒŠå–œ|å‹åŠ›å¤§|å­¤ç‹¬|å¯‚å¯|æ²®ä¸§|å¤±æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«|çƒ¦æ¼|éƒé—·|å§”å±ˆ|å«‰å¦’|ç¾¡æ…•|å†…ç–š|åæ‚”|ç¾æ„§|è‡ªè±ª|æ»¡è¶³|æ”¾æ¾|å¹³é™|å®‰å¿ƒ|æ„Ÿæ¿€|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|èˆ’æœ|è‡ªåœ¨|è‡ªä¿¡|ä¹è§‚|æ‚²è§‚|æ¶ˆæ|ç§¯æ|ç´§å¼ |ç–²åŠ³|ç´¯|å›°|é¥¿|æ¸´|ç—›|ç—’|å†·|çƒ­|ä¸èˆ’æœ)?[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|äº²å¯†|å†²çª|å®¶åº­|æœ‹å‹|çˆ±æƒ…|å­¤ç‹¬|å…³ç³»|è¡¨è¾¾|å€¾è¯‰)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
#             r"(å¦‚ä½•|æ€æ ·|æ€ä¹ˆ|ä»€ä¹ˆ|ä¸ºä»€ä¹ˆ|æ˜¯ä¸æ˜¯|èƒ½å¦|è¦ä¸è¦|è¯¥ä¸è¯¥|å€¼ä¸å€¼|æœ‰æ²¡æœ‰|èƒ½ä¸èƒ½|ä¼šä¸ä¼š|æƒ³ä¸æƒ³|æ„¿ä¸æ„¿)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(å¤„ç†|è§£å†³|é¢å¯¹|è¡¨è¾¾|æ²Ÿé€š|æ”¹å–„|ä¿®å¤|çæƒœ|å¿˜è®°|æ€€å¿µ|å›å¿†)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|æ‹çˆ±|å…³ç³»|å®¶åº­|çˆ¶æ¯|æœ‹å‹|å©šå§»|çˆ±æƒ…)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
#             r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬)?(çš„)?(ç”·å‹|å¥³æœ‹å‹|è€å©†|ä¸ˆå¤«|ä¼´ä¾£|å‰ä»»|çˆ¶æ¯|æœ‹å‹|æ‹äºº|å¯¹è±¡|è€å¸ˆ|å­©å­)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(ä¸ç†|ä¼¤å®³|æ¬ºéª—|åµæ¶|åˆ†æ‰‹|å†·æˆ˜|å¿½è§†|ç¦»å©š|å‹åŠ›|ä¼¤å¿ƒ|çº ç»“|å›°æƒ‘)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]"
#         ]
#
#     def extract_last_user_turn(self, full_text: str) -> str:
#         """ä»æ–‡æœ¬ä¸­æå–æœ€åä¸€ä¸ªç”¨æˆ·å¯¹è¯è½®æ¬¡"""
#         matches = re.findall(r"<\|im_start\|>(.*?)<\|im_end\|>", full_text, re.DOTALL)
#         if matches:
#             return matches[-1].strip().lower()
#         return full_text.strip().lower()
#
#     def classify_topic(self, text: str) -> str:
#         """å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œè¿”å›ä¸»é¢˜æ ‡ç­¾"""
#         # æ£€æŸ¥æ˜¯å¦ä¸ºæƒ…æ„Ÿä¸»é¢˜
#         if any(re.search(p, text) for p in self.emotional_patterns):
#             return "emotional"
#
#         # æ£€æŸ¥å…¶ä»–ä¸»é¢˜
#         for topic_name, keywords in self.topic_keywords.items():
#             if any(k in text for k in keywords):
#                 return topic_name
#
#         return "general"
#
#     def _analyze_dataset(self, line_iterator, total_lines, mode="full"):
#         """åˆ†ææ•°æ®é›†çš„æ ¸å¿ƒå‡½æ•°ï¼ˆæ”¯æŒå…¨é‡å’ŒæŠ½æ ·ï¼‰"""
#         topic_count = defaultdict(int)
#         total_samples = 0
#         start_time = time.time()
#
#         # è¿›åº¦æ¡æè¿°
#         desc = "åˆ†æå®Œæ•´æ•°æ®é›†" if mode == "full" else f"åˆ†ææŠ½æ ·æ•°æ® ({self.sample_size}æ ·æœ¬)"
#
#         try:
#             with tqdm(total=total_lines, desc=desc, unit="è¡Œ") as progress_bar:
#                 for line in line_iterator:
#                     try:
#                         data = json.loads(line)
#                         text = self.extract_last_user_turn(data.get('text', ''))
#                         topic = self.classify_topic(text)
#                         topic_count[topic] += 1
#                         total_samples += 1
#                         progress_bar.update(1)
#                     except json.JSONDecodeError:
#                         print(f"  ! JSONè§£æé”™è¯¯: {line[:100]}...")
#                     except Exception as e:
#                         print(f"  ! å¤„ç†é”™è¯¯: {str(e)}")
#
#                     # å¦‚æœæ˜¯æŠ½æ ·æ¨¡å¼ä¸”è¾¾åˆ°æ ·æœ¬é‡ï¼Œæå‰ç»“æŸ
#                     if mode == "sample" and total_samples >= self.sample_size:
#                         break
#         except Exception as e:
#             print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {str(e)}")
#             return None
#
#         # è®¡ç®—åˆ†æè€—æ—¶
#         elapsed_time = time.time() - start_time
#         lines_per_sec = total_samples / elapsed_time if elapsed_time > 0 else 0
#
#         # è®¡ç®—ç™¾åˆ†æ¯”å’Œç†µ
#         topic_percentages = {topic: count / total_samples * 100 for topic, count in topic_count.items()}
#         probs = [count / total_samples for count in topic_count.values()] if total_samples > 0 else []
#         entropy_val = -sum(p * math.log(p + 1e-10) for p in probs) if probs else 0
#
#         # ç”Ÿæˆç»“æœå­—å…¸
#         result_key = "full_analysis" if mode == "full" else "sample_analysis"
#         self.results[result_key] = {
#             'mode': mode,
#             'sample_size': total_samples,
#             'processing_time': elapsed_time,
#             'lines_per_sec': lines_per_sec,
#             'entropy': entropy_val,
#             'topic_counts': dict(topic_count),
#             'topic_percentages': topic_percentages
#         }
#
#         return topic_percentages
#
#     def analyze_full_dataset(self):
#         """åˆ†ææ•´ä¸ªæ•°æ®é›†çš„ä¸»é¢˜åˆ†å¸ƒ"""
#         if not os.path.exists(self.data_path):
#             print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
#             return None
#
#         print(f"\nâ–¶ å¼€å§‹åˆ†æå®Œæ•´æ•°æ®é›†: {self.data_path}")
#         print(f"  æ–‡ä»¶å¤§å°: {self.file_size / (1024 * 1024):.2f} MB")
#
#         # è·å–æ–‡ä»¶æ€»è¡Œæ•°
#         with open(self.data_path, 'r', encoding='utf-8') as f:
#             total_lines = sum(1 for _ in f)
#
#         # é‡æ–°æ‰“å¼€æ–‡ä»¶è¿›è¡Œå¤„ç†
#         with open(self.data_path, 'r', encoding='utf-8') as f:
#             topic_percentages = self._analyze_dataset(f, total_lines, mode="full")
#
#         # æ‰“å°æŠ¥å‘Š
#         self._print_analysis_report("full_analysis")
#         return topic_percentages
#
#     def analyze_sampled_dataset(self, sample_size=None):
#         """éšæœºæŠ½æ ·åˆ†ææ•°æ®é›†ä¸»é¢˜åˆ†å¸ƒ"""
#         if sample_size is not None:
#             self.sample_size = sample_size
#
#         if not os.path.exists(self.data_path):
#             print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
#             return None
#
#         print(f"\nâ–¶ å¼€å§‹éšæœºæŠ½æ ·åˆ†ææ•°æ®é›†: {self.data_path}")
#         print(f"  æŠ½æ ·å¤§å°: {self.sample_size:,} è¡Œ")
#
#         # è·å–æ–‡ä»¶æ€»è¡Œæ•°
#         with open(self.data_path, 'r', encoding='utf-8') as f:
#             total_lines = sum(1 for _ in f)
#
#         # åˆ›å»ºéšæœºè¡Œç´¢å¼•ç”Ÿæˆå™¨
#         def random_line_generator():
#             with open(self.data_path, 'r', encoding='utf-8') as f:
#                 # åˆ›å»ºè¡Œç´¢å¼•åˆ—è¡¨å¹¶æ‰“ä¹±
#                 indices = list(range(total_lines))
#                 random.shuffle(indices)
#
#                 # åªå–éœ€è¦çš„æ ·æœ¬é‡
#                 indices = indices[:self.sample_size]
#
#                 # æŒ‰è¡Œå·æ’åºä»¥ä¾¿é¡ºåºè¯»å–
#                 indices.sort()
#
#                 # é¡ºåºè¯»å–æ–‡ä»¶è¡Œ
#                 for i, line in enumerate(f):
#                     if i in indices:
#                         yield line
#
#         # è¿›è¡Œåˆ†æ
#         topic_percentages = self._analyze_dataset(random_line_generator(), min(self.sample_size, total_lines),
#                                                   mode="sample")
#
#         # æ‰“å°æŠ¥å‘Š
#         self._print_analysis_report("sample_analysis")
#         return topic_percentages
#
#     def _print_analysis_report(self, result_key):
#         """æ‰“å°åˆ†ææŠ¥å‘Š"""
#         if result_key not in self.results:
#             print("âŒ æ²¡æœ‰å¯ç”¨çš„åˆ†æç»“æœ")
#             return
#
#         result = self.results[result_key]
#         total_samples = result['sample_size']
#         topic_percentages = result['topic_percentages']
#         topic_count = result['topic_counts']
#
#         mode = "å®Œæ•´åˆ†æ" if result['mode'] == "full" else f"æŠ½æ ·åˆ†æ ({self.sample_size}æ ·æœ¬)"
#
#         print("\n" + "=" * 60)
#         print(f"ğŸ“Š æ•°æ®é›†åˆ†ææŠ¥å‘Š - {mode}")
#         print(f"  æ€»æ ·æœ¬æ•°: {total_samples:,}")
#         print(f"  åˆ†æè€—æ—¶: {result['processing_time']:.2f} ç§’ ({result['lines_per_sec']:.1f} è¡Œ/ç§’)")
#         print(f"  ä¸»é¢˜ç†µå€¼: {result['entropy']:.4f} (è¡¡é‡ä¸»é¢˜å¤šæ ·æ€§)")
#         print("-" * 60)
#         print(f"  {'ä¸»é¢˜':<15} {'æ•°é‡':>12} {'å æ¯”':>10} {'æŸ±çŠ¶å›¾':<20}")
#         print("-" * 60)
#
#         # æŒ‰å æ¯”æ’åºå¹¶æ‰“å°æŸ±çŠ¶å›¾
#         max_count = max(topic_count.values()) if topic_count else 1
#         for topic, count in sorted(topic_count.items(), key=lambda x: x[1], reverse=True):
#             percentage = topic_percentages[topic]
#             bar_length = int(50 * count / max_count)
#             bar = 'â–ˆ' * bar_length
#             print(f"  {topic:<15} {count:>12,} {percentage:>9.2f}%  {bar}")
#
#         print("=" * 60)
#
#         # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
#         self.generate_visual_report(topic_percentages, result['mode'])
#
#     def generate_visual_report(self, topic_percentages, mode):
#         """ç”Ÿæˆå¯è§†åŒ–çš„ä¸»é¢˜åˆ†å¸ƒæŠ¥å‘Š"""
#         try:
#             import matplotlib.pyplot as plt
#             import numpy as np
#
#             print("\nâ–¶ ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
#
#             # å‡†å¤‡æ•°æ®
#             labels = list(topic_percentages.keys())
#             sizes = list(topic_percentages.values())
#             sorted_indices = np.argsort(sizes)[::-1]
#             labels = [labels[i] for i in sorted_indices]
#             sizes = [sizes[i] for i in sorted_indices]
#
#             # åˆ›å»ºé¥¼å›¾
#             plt.figure(figsize=(12, 8))
#             explode = [0.1 if i == 0 else 0 for i in range(len(labels))]
#             plt.pie(sizes, labels=labels, autopct='%1.1f%%',
#                     startangle=90, explode=explode, shadow=True)
#             plt.axis('equal')
#             mode_str = "å®Œæ•´æ•°æ®é›†" if mode == "full" else f"æŠ½æ ·æ•°æ® ({self.sample_size}æ ·æœ¬)"
#             plt.title(f'æ•°æ®é›†ä¸»é¢˜åˆ†å¸ƒ - {mode_str}')
#
#             # ä¿å­˜å›¾è¡¨
#             chart_suffix = "_full" if mode == "full" else f"_sample_{self.sample_size}"
#             chart_path = os.path.join(os.path.dirname(self.data_path), f"dataset_topic_distribution{chart_suffix}.png")
#             plt.savefig(chart_path, dpi=300, bbox_inches='tight')
#             print(f"âœ… é¥¼å›¾å·²ä¿å­˜è‡³: {chart_path}")
#             plt.close()
#
#             # åˆ›å»ºæŸ±çŠ¶å›¾
#             plt.figure(figsize=(14, 8))
#             colors = plt.cm.tab20(np.linspace(0, 1, len(labels)))
#             bars = plt.bar(labels, sizes, color=colors)
#
#             # æ·»åŠ æ•°å€¼æ ‡ç­¾
#             for bar in bars:
#                 height = bar.get_height()
#                 plt.text(bar.get_x() + bar.get_width() / 2., height,
#                          f'{height:.1f}%', ha='center', va='bottom')
#
#             plt.xlabel('ä¸»é¢˜ç±»åˆ«')
#             plt.ylabel('å æ¯” (%)')
#             plt.title(f'æ•°æ®é›†ä¸»é¢˜åˆ†å¸ƒ - {mode_str}')
#             plt.xticks(rotation=45)
#             plt.grid(axis='y', linestyle='--', alpha=0.7)
#
#             # ä¿å­˜æŸ±çŠ¶å›¾
#             bar_chart_path = os.path.join(os.path.dirname(self.data_path), f"dataset_topic_barchart{chart_suffix}.png")
#             plt.savefig(bar_chart_path, dpi=300, bbox_inches='tight')
#             print(f"âœ… æŸ±çŠ¶å›¾å·²ä¿å­˜è‡³: {bar_chart_path}")
#             plt.close()
#
#         except ImportError:
#             print("âš ï¸ æ— æ³•ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šï¼Œè¯·å®‰è£…matplotlib: pip install matplotlib")
#         except Exception as e:
#             print(f"âš ï¸ ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
#
#     def save_results(self, file_path=None):
#         """ä¿å­˜åˆ†æç»“æœåˆ°æ–‡ä»¶"""
#         if not self.results:
#             print("âŒ æ²¡æœ‰å¯ç”¨çš„åˆ†æç»“æœ")
#             return False
#
#         if file_path is None:
#             file_dir = os.path.dirname(self.data_path)
#             file_name = os.path.basename(self.data_path).split('.')[0] + "_analysis.json"
#             file_path = os.path.join(file_dir, file_name)
#
#         try:
#             with open(file_path, 'w', encoding='utf-8') as f:
#                 json.dump(self.results, f, ensure_ascii=False, indent=2)
#             print(f"âœ… åˆ†æç»“æœå·²ä¿å­˜è‡³: {file_path}")
#             return True
#         except Exception as e:
#             print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")
#             return False
#
#
# if __name__ == "__main__":
#     # åˆ›å»ºè¯„ä¼°å™¨å®ä¾‹
#     evaluator = DataSetEvaluator(
#         data_path="D:/PythonCode/RainLLM/dataset/new_pre_hq.jsonl",
#         sample_size=500000,
#         shuffle_sample_size=10000
#     )
#
#     # ä½¿ç”¨ç¤ºä¾‹ï¼šæŠ½æ ·åˆ†æ
#     evaluator.analyze_full_dataset()
#     # evaluator.analyze_sampled_dataset(sample_size=10000)
#     evaluator.save_results()
#
#     # ä½¿ç”¨ç¤ºä¾‹ï¼šå…¨é‡åˆ†æï¼ˆå¤§å‹æ•°æ®é›†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
#     # evaluator.analyze_full_dataset()
#     # evaluator.save_results()
import json
import re
import os
import time
import math
from collections import defaultdict
from tqdm import tqdm
import multiprocessing


class DataSetEvaluatorMultiProcess:
    def __init__(self, data_path, sample_size=1000000, num_workers=None):
        self.data_path = data_path
        self.sample_size = sample_size
        self.num_workers = num_workers or max(1, multiprocessing.cpu_count() - 1)
        self.results = {}
        self.file_size = os.path.getsize(data_path) if os.path.exists(data_path) else 0

        self.topic_keywords = {
            "medical": ["åŒ»å­¦", "åŒ»ç–—", "å¥åº·", "ç–¾ç—…", "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‚£è€…", "æ²»ç–—", "è¯Šæ–­", "è¯ç‰©", "æ‰‹æœ¯"],
            "code": ["ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "å˜é‡", "ç±»", "æ–¹æ³•", "ç®—æ³•", "æ•°æ®åº“", "API", "ç¼–ç¨‹è¯­è¨€", "è°ƒè¯•", "æ¡†æ¶"],
            "finance": ["é‡‘è", "è‚¡ç¥¨", "æŠ•èµ„", "é“¶è¡Œ", "ç»æµ", "å¸‚åœº", "äº¤æ˜“", "è´§å¸", "æ±‡ç‡", "åŸºé‡‘", "ç†è´¢"],
            "education": ["æ•™è‚²", "å­¦æ ¡", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™å­¦", "è€å¸ˆ", "å­¦ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "æ•™æ", "å­¦ä½"],
            "tech": ["æŠ€æœ¯", "ç§‘æŠ€", "åˆ›æ–°", "ç ”å‘", "å·¥ç¨‹", "è®¾å¤‡", "ç³»ç»Ÿ", "åº”ç”¨", "æ™ºèƒ½", "æœºå™¨äºº", "äººå·¥æ™ºèƒ½"],
            "entertainment": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ¸¸æˆ", "æ˜æ˜Ÿ", "èŠ‚ç›®", "æ¼”å‡º", "ç»¼è‰º", "è‰ºäºº", "æ¼”å”±ä¼š", "ç”µè§†å‰§"],
            "emotional": ["æƒ…æ„Ÿ", "å¿ƒæƒ…", "æ„Ÿå—", "çˆ±", "å–œæ¬¢", "æ‹çˆ±", "åˆ†æ‰‹", "å®‰æ…°", "æ”¯æŒ", "ç†è§£", "å…³ç³»",
                          "æœ‹å‹", "å®¶äºº", "å­¤ç‹¬", "å¼€å¿ƒ", "ä¼¤å¿ƒ", "æ„¤æ€’", "å‹åŠ›", "å¿ƒç†å’¨è¯¢", "æƒ…ç»ªç®¡ç†", "å…±æƒ…",
                          "äº²å¯†å…³ç³»", "æƒ…æ„Ÿè¡¨è¾¾", "å¿ƒç†æ”¯æŒ", "æƒ…æ„Ÿéœ€æ±‚", "æƒ…æ„Ÿå›°æƒ‘", "æƒ…æ„Ÿäº¤æµ", "æƒ…æ„Ÿå’¨è¯¢"],
            "science": ["ç§‘å­¦", "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©", "æ•°å­¦", "å¤©æ–‡", "åœ°ç†", "å®éªŒ", "ç†è®º", "ç ”ç©¶", "è‡ªç„¶"],
            "literature": ["æ–‡å­¦", "å°è¯´", "è¯—æ­Œ", "æ•£æ–‡", "ä½œå®¶", "ä½œå“", "é˜…è¯»", "å†™ä½œ", "æ•…äº‹", "æƒ…èŠ‚", "äººç‰©"],
            "sports": ["ä½“è‚²", "è¿åŠ¨", "æ¯”èµ›", "è¶³çƒ", "ç¯®çƒ", "è¿åŠ¨å‘˜", "è®­ç»ƒ", "å¥¥è¿", "å† å†›", "èµ›äº‹", "å¥èº«"]
        }

        self.emotional_patterns = [
            r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(æ„Ÿåˆ°|è§‰å¾—|æ„Ÿè§‰|è®¤ä¸º|ä»¥ä¸º|å‘ç°|çŸ¥é“|å¸Œæœ›|æƒ³è¦|éœ€è¦|å–œæ¬¢|çˆ±|è®¨åŒ|æ¨|å®³æ€•|æ‹…å¿ƒ|ç„¦è™‘|ç”Ÿæ°”|æ„¤æ€’|ä¼¤å¿ƒ|éš¾è¿‡|å¼€å¿ƒ|å¿«ä¹|å…´å¥‹|æƒŠå–œ|å‹åŠ›å¤§|å­¤ç‹¬|å¯‚å¯|æ²®ä¸§|å¤±æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«|çƒ¦æ¼|éƒé—·|å§”å±ˆ|å«‰å¦’|ç¾¡æ…•|å†…ç–š|åæ‚”|ç¾æ„§|è‡ªè±ª|æ»¡è¶³|æ”¾æ¾|å¹³é™|å®‰å¿ƒ|æ„Ÿæ¿€|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|èˆ’æœ|è‡ªåœ¨|è‡ªä¿¡|ä¹è§‚|æ‚²è§‚|æ¶ˆæ|ç§¯æ|ç´§å¼ |ç–²åŠ³|ç´¯|å›°|é¥¿|æ¸´|ç—›|ç—’|å†·|çƒ­|ä¸èˆ’æœ)?[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|äº²å¯†|å†²çª|å®¶åº­|æœ‹å‹|çˆ±æƒ…|å­¤ç‹¬|å…³ç³»|è¡¨è¾¾|å€¾è¯‰)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
            r"(å¦‚ä½•|æ€æ ·|æ€ä¹ˆ|ä»€ä¹ˆ|ä¸ºä»€ä¹ˆ|æ˜¯ä¸æ˜¯|èƒ½å¦|è¦ä¸è¦|è¯¥ä¸è¯¥|å€¼ä¸å€¼|æœ‰æ²¡æœ‰|èƒ½ä¸èƒ½|ä¼šä¸ä¼š|æƒ³ä¸æƒ³|æ„¿ä¸æ„¿)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(å¤„ç†|è§£å†³|é¢å¯¹|è¡¨è¾¾|æ²Ÿé€š|æ”¹å–„|ä¿®å¤|çæƒœ|å¿˜è®°|æ€€å¿µ|å›å¿†)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|æ‹çˆ±|å…³ç³»|å®¶åº­|çˆ¶æ¯|æœ‹å‹|å©šå§»|çˆ±æƒ…)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
            r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬)?(çš„)?(ç”·å‹|å¥³æœ‹å‹|è€å©†|ä¸ˆå¤«|ä¼´ä¾£|å‰ä»»|çˆ¶æ¯|æœ‹å‹|æ‹äºº|å¯¹è±¡|è€å¸ˆ|å­©å­)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(ä¸ç†|ä¼¤å®³|æ¬ºéª—|åµæ¶|åˆ†æ‰‹|å†·æˆ˜|å¿½è§†|ç¦»å©š|å‹åŠ›|ä¼¤å¿ƒ|çº ç»“|å›°æƒ‘)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]"
        ]

    def extract_last_user_turn(self, full_text: str) -> str:
        matches = re.findall(r"<\|im_start\|>(.*?)<\|im_end\|>", full_text, re.DOTALL)
        if matches:
            return matches[-1].strip().lower()
        return full_text.strip().lower()

    def classify_topic(self, text: str) -> str:
        if any(re.search(p, text) for p in self.emotional_patterns):
            return "emotional"
        for topic_name, keywords in self.topic_keywords.items():
            if any(k in text for k in keywords):
                return topic_name
        return "general"

    def worker(self, lines_chunk):
        topic_count = defaultdict(int)
        processed = 0
        for line in lines_chunk:
            try:
                data = json.loads(line)
                text = self.extract_last_user_turn(data.get('text', ''))
                topic = self.classify_topic(text)
                topic_count[topic] += 1
                processed += 1
            except:
                pass
        return topic_count, processed

    def analyze_full_dataset(self):
        if not os.path.exists(self.data_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
            return None

        print(f"\nâ–¶ å…¨æ•°æ®é›†å¹¶è¡Œåˆ†æ: {self.data_path}")
        print(f"  æ–‡ä»¶å¤§å°: {self.file_size / (1024 * 1024):.2f} MB")

        with open(self.data_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        total_lines = len(lines)
        chunk_size = max(1, total_lines // self.num_workers)
        chunks = [lines[i:i + chunk_size] for i in range(0, total_lines, chunk_size)]

        start_time = time.time()
        with multiprocessing.Pool(self.num_workers) as pool:
            results = list(tqdm(pool.imap(self.worker, chunks), total=len(chunks)))

        merged_topic_count = defaultdict(int)
        total_processed = 0
        for topic_count, processed in results:
            total_processed += processed
            for k, v in topic_count.items():
                merged_topic_count[k] += v

        elapsed_time = time.time() - start_time
        lines_per_sec = total_processed / elapsed_time if elapsed_time > 0 else 0

        topic_percentages = {topic: count / total_processed * 100 for topic, count in merged_topic_count.items()}
        probs = [count / total_processed for count in merged_topic_count.values()] if total_processed > 0 else []
        entropy_val = -sum(p * math.log(p + 1e-10) for p in probs) if probs else 0

        self.results['multi_process_full_analysis'] = {
            'mode': 'full',
            'sample_size': total_processed,
            'processing_time': elapsed_time,
            'lines_per_sec': lines_per_sec,
            'entropy': entropy_val,
            'topic_counts': dict(merged_topic_count),
            'topic_percentages': topic_percentages
        }

        self._print_analysis_report('multi_process_full_analysis')
        return topic_percentages

    def _print_analysis_report(self, result_key):
        if result_key not in self.results:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„åˆ†æç»“æœ")
            return

        result = self.results[result_key]
        total_samples = result['sample_size']
        topic_percentages = result['topic_percentages']
        topic_count = result['topic_counts']

        mode = "å®Œæ•´åˆ†æ" if result['mode'] == "full" else f"æŠ½æ ·åˆ†æ ({self.sample_size}æ ·æœ¬)"

        print("\n" + "=" * 60)
        print(f"ğŸ“Š æ•°æ®é›†åˆ†ææŠ¥å‘Š - {mode}")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples:,}")
        print(f"  åˆ†æè€—æ—¶: {result['processing_time']:.2f} ç§’ ({result['lines_per_sec']:.1f} è¡Œ/ç§’)")
        print(f"  ä¸»é¢˜ç†µå€¼: {result['entropy']:.4f} (è¡¡é‡ä¸»é¢˜å¤šæ ·æ€§)")
        print("-" * 60)
        print(f"  {'ä¸»é¢˜':<15} {'æ•°é‡':>12} {'å æ¯”':>10} {'æŸ±çŠ¶å›¾':<20}")
        print("-" * 60)

        max_count = max(topic_count.values()) if topic_count else 1
        for topic, count in sorted(topic_count.items(), key=lambda x: x[1], reverse=True):
            percentage = topic_percentages[topic]
            bar_length = int(50 * count / max_count)
            bar = 'â–ˆ' * bar_length
            print(f"  {topic:<15} {count:>12,} {percentage:>9.2f}%  {bar}")

        print("=" * 60)


if __name__ == "__main__":
    evaluator = DataSetEvaluatorMultiProcess(
        data_path="D:/PythonCode/RainLLM/dataset/sft_512.jsonl",
        sample_size=500000,
        num_workers=16  # æ ¹æ®ä½ çš„CPUæ ¸å¿ƒæ•°è°ƒæ•´
    )

    evaluator.analyze_full_dataset()
