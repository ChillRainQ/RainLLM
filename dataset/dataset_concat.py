# # import json
# # from pathlib import Path
# #
# # from tqdm import tqdm
# #
# #
# # def merge_datasets(dataset1_path, dataset2_path, output_path):
# #     """åˆå¹¶ä¸¤ä¸ªJSONLæ ¼å¼çš„é¢„è®­ç»ƒæ•°æ®é›†"""
# #     dataset1 = Path(dataset1_path)
# #     dataset2 = Path(dataset2_path)
# #     output = Path(output_path)
# #
# #     # éªŒè¯æ–‡ä»¶å­˜åœ¨
# #     if not dataset1.exists() or not dataset2.exists():
# #         raise FileNotFoundError("æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨")
# #
# #     # ç»Ÿè®¡è¡Œæ•°
# #     total_lines = sum(1 for _ in open(dataset1, 'r', encoding='utf-8')) + \
# #                   sum(1 for _ in open(dataset2, 'r', encoding='utf-8'))
# #
# #     # åˆå¹¶æ–‡ä»¶
# #     with open(output, 'w', encoding='utf-8') as out_f:
# #         # æ•°æ®é›†1
# #         with open(dataset1, 'r', encoding='utf-8') as f1:
# #             for line in tqdm(f1, total=total_lines // 2, desc="åˆå¹¶æ•°æ®é›†1"):
# #                 try:
# #                     data = json.loads(line)
# #                     if 'text' in data and data['text'].strip():
# #                         out_f.write(line)
# #                 except json.JSONDecodeError:
# #                     continue
# #
# #         # æ•°æ®é›†2
# #         with open(dataset2, 'r', encoding='utf-8') as f2:
# #             for line in tqdm(f2, total=total_lines // 2, desc="åˆå¹¶æ•°æ®é›†2"):
# #                 try:
# #                     data = json.loads(line)
# #                     if 'text' in data and data['text'].strip():
# #                         out_f.write(line)
# #                 except json.JSONDecodeError:
# #                     continue
# #
# #     print(f"âœ… æ•°æ®é›†åˆå¹¶å®Œæˆï¼Œä¿å­˜è‡³: {output_path}")
# #     return output_path
# #
# #
# # # ä½¿ç”¨ç¤ºä¾‹
# # merged_path = merge_datasets(
# #     dataset1_path="pretrain_hq.jsonl",
# #     dataset2_path="pretrain_hq_v7.jsonl",
# #     output_path="pretrain_combined.jsonl"
# # )
# import json
# import random
# from pathlib import Path
# import shutil
#
#
# def shuffle_large_jsonl(input_path, output_path, buffer_size=100000):
#     """æ‰“ä¹±å¤§å‹JSONLæ•°æ®é›†"""
#     input_file = Path(input_path)
#     output_file = Path(output_path)
#
#     # ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡æ€»è¡Œæ•°
#     print("â–¶ ç»Ÿè®¡æ•°æ®é›†è¡Œæ•°...")
#     total_lines = 0
#     with open(input_file, 'r', encoding='utf-8') as f:
#         for _ in f:
#             total_lines += 1
#
#     # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºè¡Œç´¢å¼•
#     print("â–¶ åˆ›å»ºè¡Œç´¢å¼•...")
#     line_indices = list(range(total_lines))
#     random.shuffle(line_indices)
#
#     # ç¬¬ä¸‰æ­¥ï¼šåˆ†å—æ‰“ä¹±
#     print("â–¶ åˆ†å—æ‰“ä¹±æ•°æ®...")
#     temp_dir = output_file.parent / "temp_shuffle"
#     temp_dir.mkdir(exist_ok=True)
#
#     # åˆ†å—è¯»å–å’Œæ‰“ä¹±
#     buffer = []
#     chunk_count = 0
#     with open(input_file, 'r', encoding='utf-8') as f:
#         for idx, line in enumerate(f):
#             buffer.append((line_indices[idx], line))
#
#             if len(buffer) >= buffer_size:
#                 buffer.sort(key=lambda x: x[0])  # æŒ‰éšæœºç´¢å¼•æ’åº
#                 chunk_path = temp_dir / f"chunk_{chunk_count}.jsonl"
#                 with open(chunk_path, 'w', encoding='utf-8') as chunk_f:
#                     for _, line_text in buffer:
#                         chunk_f.write(line_text)
#                 chunk_count += 1
#                 buffer = []
#
#     # å¤„ç†å‰©ä½™è¡Œ
#     if buffer:
#         buffer.sort(key=lambda x: x[0])
#         chunk_path = temp_dir / f"chunk_{chunk_count}.jsonl"
#         with open(chunk_path, 'w', encoding='utf-8') as chunk_f:
#             for _, line_text in buffer:
#                 chunk_f.write(line_text)
#         chunk_count += 1
#
#     # ç¬¬å››æ­¥ï¼šåˆå¹¶åˆ†å—
#     print("â–¶ åˆå¹¶æ‰“ä¹±åçš„æ•°æ®...")
#     with open(output_file, 'w', encoding='utf-8') as out_f:
#         for i in range(chunk_count):
#             chunk_path = temp_dir / f"chunk_{i}.jsonl"
#             with open(chunk_path, 'r', encoding='utf-8') as chunk_f:
#                 shutil.copyfileobj(chunk_f, out_f)
#             chunk_path.unlink()
#
#     # æ¸…ç†ä¸´æ—¶ç›®å½•
#     temp_dir.rmdir()
#     print(f"âœ… æ•°æ®é›†å·²æ‰“ä¹±å¹¶ä¿å­˜è‡³: {output_path}")
#     return output_path
#
#
# if __name__ == "__main__":
#     # æ‰“ä¹±åˆå¹¶æ•°æ®é›†
#     shuffled_path = shuffle_large_jsonl(
#         input_path=r"D:\PythonCode\RainLLM\dataset\pretrain_combined.jsonl",
#         output_path=r"D:\PythonCode\RainLLM\dataset\pretrain_combined_shuffled.jsonl"
#     )
import json
import random
import shutil
import re
import math
import time
from pathlib import Path
from collections import defaultdict


def validate_shuffle(dataset_path, sample_size=10000):
    """éªŒè¯æ•°æ®é›†æ˜¯å¦å……åˆ†æ‰“ä¹±ï¼ˆä¼˜åŒ–æƒ…æ„Ÿå¯¹è¯æ£€æµ‹ï¼‰"""
    print("â–¶ éªŒè¯æ•°æ®é›†æ‰“ä¹±è´¨é‡...")
    topic_sequence = []
    topic_count = defaultdict(int)

    # æ‰©å±•ä¸»é¢˜å…³é”®è¯ï¼ˆå¢åŠ æƒ…æ„Ÿå¯¹è¯ç±»åˆ«ï¼‰
    topic_keywords = {
        "medical": ["åŒ»å­¦", "åŒ»ç–—", "å¥åº·", "ç–¾ç—…", "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‚£è€…", "æ²»ç–—", "è¯Šæ–­", "è¯ç‰©", "æ‰‹æœ¯"],
        "code": ["ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "å˜é‡", "ç±»", "æ–¹æ³•", "ç®—æ³•", "æ•°æ®åº“", "API", "ç¼–ç¨‹è¯­è¨€", "è°ƒè¯•", "æ¡†æ¶"],
        "finance": ["é‡‘è", "è‚¡ç¥¨", "æŠ•èµ„", "é“¶è¡Œ", "ç»æµ", "å¸‚åœº", "äº¤æ˜“", "è´§å¸", "æ±‡ç‡", "åŸºé‡‘", "ç†è´¢"],
        "education": ["æ•™è‚²", "å­¦æ ¡", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™å­¦", "è€å¸ˆ", "å­¦ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "æ•™æ", "å­¦ä½"],
        "tech": ["æŠ€æœ¯", "ç§‘æŠ€", "åˆ›æ–°", "ç ”å‘", "å·¥ç¨‹", "è®¾å¤‡", "ç³»ç»Ÿ", "åº”ç”¨", "æ™ºèƒ½", "æœºå™¨äºº", "äººå·¥æ™ºèƒ½"],
        "entertainment": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ¸¸æˆ", "æ˜æ˜Ÿ", "èŠ‚ç›®", "æ¼”å‡º", "ç»¼è‰º", "è‰ºäºº", "æ¼”å”±ä¼š", "ç”µè§†å‰§"],
        "emotional": ["æƒ…æ„Ÿ", "å¿ƒæƒ…", "æ„Ÿå—", "çˆ±", "å–œæ¬¢", "æ‹çˆ±", "åˆ†æ‰‹", "å®‰æ…°", "æ”¯æŒ", "ç†è§£", "å…³ç³»",
                      "æœ‹å‹", "å®¶äºº", "å­¤ç‹¬", "å¼€å¿ƒ", "ä¼¤å¿ƒ", "æ„¤æ€’", "å‹åŠ›", "å¿ƒç†å’¨è¯¢", "æƒ…ç»ªç®¡ç†", "å…±æƒ…",
                      "äº²å¯†å…³ç³»", "æƒ…æ„Ÿè¡¨è¾¾", "å¿ƒç†æ”¯æŒ", "æƒ…æ„Ÿéœ€æ±‚", "æƒ…æ„Ÿå›°æƒ‘", "æƒ…æ„Ÿäº¤æµ", "æƒ…æ„Ÿå’¨è¯¢"]
    }

    # æƒ…æ„Ÿå¯¹è¯ä¸“ç”¨æ£€æµ‹æ¨¡å¼ - æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
    emotional_patterns = [
        r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(æ„Ÿåˆ°|è§‰å¾—|æ„Ÿè§‰|è®¤ä¸º|ä»¥ä¸º|å‘ç°|çŸ¥é“|å¸Œæœ›|æƒ³è¦|éœ€è¦|å–œæ¬¢|çˆ±|è®¨åŒ|æ¨|å®³æ€•|æ‹…å¿ƒ|ç„¦è™‘|ç”Ÿæ°”|æ„¤æ€’|ä¼¤å¿ƒ|éš¾è¿‡|å¼€å¿ƒ|å¿«ä¹|å…´å¥‹|æƒŠå–œ|å‹åŠ›å¤§|å­¤ç‹¬|å¯‚å¯|æ²®ä¸§|å¤±æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«|çƒ¦æ¼|éƒé—·|å§”å±ˆ|å«‰å¦’|ç¾¡æ…•|å†…ç–š|åæ‚”|ç¾æ„§|è‡ªè±ª|æ»¡è¶³|æ”¾æ¾|å¹³é™|å®‰å¿ƒ|æ„Ÿæ¿€|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|èˆ’æœ|è‡ªåœ¨|è‡ªä¿¡|ä¹è§‚|æ‚²è§‚|æ¶ˆæ|ç§¯æ|ç´§å¼ |ç–²åŠ³|ç´¯|å›°|é¥¿|æ¸´|ç—›|ç—’|å†·|çƒ­|ä¸èˆ’æœ)?[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|å«‰å¦’|ç¾¡æ…•|äº²å¯†|ç–è¿œ|å†·æ·¡|çƒ­æƒ…|å…³å¿ƒ|ä½“è´´|ç†è§£|æ”¯æŒ|å®‰æ…°|é¼“åŠ±|å¸®åŠ©|é™ªä¼´|å€¾å¬|å€¾è¯‰|åˆ†äº«|æ²Ÿé€š|äº¤æµ|è¡¨è¾¾|æ²‰é»˜|å†·æˆ˜|å†²çª|çŸ›ç›¾|è§£å†³|å¤„ç†|åº”å¯¹|è°ƒèŠ‚|æ§åˆ¶|é‡Šæ”¾|å‘æ³„|å‹æŠ‘|ç§¯ç´¯|çˆ†å‘|æ¢å¤|é‡å»º|ç»´ç³»|ç»è¥|çæƒœ|æ”¾å¼ƒ|æ”¾ä¸‹|å¿˜è®°|æ€€å¿µ|æ€å¿µ|å›å¿†|è¿‡å»|ç°åœ¨|æœªæ¥|å®¶åº­|çˆ¶æ¯|å­å¥³|å…„å¼Ÿå§å¦¹|äº²æˆš|æœ‹å‹|é—ºèœœ|å“¥ä»¬|åŒäº‹|åŒå­¦|æ‹äºº|çˆ±äºº|ä¼´ä¾£|å¤«å¦»|æƒ…ä¾£|å•èº«|æ‹çˆ±å…³ç³»|å©šå§»å…³ç³»|äº²å­å…³ç³»|å‹æƒ…|äººé™…å…³ç³»|ç¤¾äº¤|å­¤ç‹¬|åˆç¾¤|æ’æ–¥|æ¥çº³|è®¤åŒ|å°Šé‡|åŒ…å®¹|å®½å®¹|ä½“è°…|å…³å¿ƒ|çˆ±æŠ¤|ä¿æŠ¤|ä¾èµ–|ç‹¬ç«‹|è‡ªç”±|æŸç¼š|æ§åˆ¶|å æœ‰|ç‰ºç‰²|ä»˜å‡º|å›æŠ¥|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|ç—›è‹¦|å¿«ä¹)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
        r"(å¦‚ä½•|æ€æ ·|æ€ä¹ˆ|ä»€ä¹ˆ|ä¸ºä»€ä¹ˆ|æ˜¯ä¸æ˜¯|èƒ½å¦|è¦ä¸è¦|è¯¥ä¸è¯¥|å€¼ä¸å€¼|æœ‰æ²¡æœ‰|èƒ½ä¸èƒ½|ä¼šä¸ä¼š|æƒ³ä¸æƒ³|æ„¿ä¸æ„¿|æ•¢ä¸æ•¢|è‚¯ä¸è‚¯)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(å¤„ç†|åº”å¯¹|è§£å†³|é¢å¯¹|çœ‹å¾…|ç†è§£|åˆ†æ|è°ƒèŠ‚|æ§åˆ¶|é‡Šæ”¾|è¡¨è¾¾|æ²Ÿé€š|äº¤æµ|æ”¹å–„|ä¿®å¤|ç»´ç³»|ç»è¥|çæƒœ|æ”¾å¼ƒ|æ”¾ä¸‹|å¿˜è®°|æ€€å¿µ|å›å¿†|é‡å»º|æ¢å¤)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|å«‰å¦’|ç¾¡æ…•|äº²å¯†|ç–è¿œ|å†·æ·¡|çƒ­æƒ…|å…³å¿ƒ|ä½“è´´|ç†è§£|æ”¯æŒ|å®‰æ…°|é¼“åŠ±|å¸®åŠ©|é™ªä¼´|å€¾å¬|å€¾è¯‰|åˆ†äº«|æ²Ÿé€š|äº¤æµ|è¡¨è¾¾|æ²‰é»˜|å†·æˆ˜|å†²çª|çŸ›ç›¾|å…³ç³»|å®¶åº­|çˆ¶æ¯|å­å¥³|å…„å¼Ÿå§å¦¹|äº²æˆš|æœ‹å‹|é—ºèœœ|å“¥ä»¬|åŒäº‹|åŒå­¦|æ‹äºº|çˆ±äºº|ä¼´ä¾£|å¤«å¦»|æƒ…ä¾£|å•èº«|æ‹çˆ±å…³ç³»|å©šå§»å…³ç³»|äº²å­å…³ç³»|å‹æƒ…|äººé™…å…³ç³»|ç¤¾äº¤|å­¤ç‹¬|åˆç¾¤|æ’æ–¥|æ¥çº³|è®¤åŒ|å°Šé‡|åŒ…å®¹|å®½å®¹|ä½“è°…|å…³å¿ƒ|çˆ±æŠ¤|ä¿æŠ¤|ä¾èµ–|ç‹¬ç«‹|è‡ªç”±|æŸç¼š|æ§åˆ¶|å æœ‰|ç‰ºç‰²|ä»˜å‡º|å›æŠ¥|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|ç—›è‹¦|å¿«ä¹)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]",
        r"(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(çš„)?(ç”·å‹|å¥³æœ‹å‹|è€å…¬|è€å©†|å¦»å­|ä¸ˆå¤«|ä¼´ä¾£|æ‹äºº|çˆ±äºº|å¯¹è±¡|å‰ä»»|å‰ç”·å‹|å‰å¥³å‹|å‰å¤«|å‰å¦»|çˆ¶æ¯|çˆ¸çˆ¸|å¦ˆå¦ˆ|çˆ¶äº²|æ¯äº²|å­©å­|å„¿å­|å¥³å„¿|å…„å¼Ÿ|å§å¦¹|å“¥å“¥|å¼Ÿå¼Ÿ|å§å§|å¦¹å¦¹|æœ‹å‹|é—ºèœœ|å“¥ä»¬|åŒäº‹|åŒå­¦|è€å¸ˆ|å­¦ç”Ÿ|é¢†å¯¼|ä¸‹å±)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(ä¸ç†|ä¸è”ç³»|ä¸å›å¤|ä¸å›æ¶ˆæ¯|ä¸æ¥ç”µè¯|ä¸è§é¢|ä¸å…³å¿ƒ|ä¸åœ¨ä¹|ä¸é‡è§†|ä¸ç†è§£|ä¸æ”¯æŒ|ä¸ä¿¡ä»»|ä¸å°Šé‡|ä¸åŒ…å®¹|ä¸ä½“è´´|ä¸çˆ±|ä¸å–œæ¬¢|è®¨åŒ|æ¨|ä¼¤å®³|æ¬ºéª—|èƒŒå›|å‡ºè½¨|åŠˆè…¿|è¯´è°|éšç’|è¯¯ä¼š|å†¤æ‰|æŒ‡è´£|æ‰¹è¯„|æŠ±æ€¨|åŸ‹æ€¨|è´£æ€ª|äº‰åµ|åµæ¶|æ‰“æ¶|å†·æˆ˜|åˆ†æ‰‹|ç¦»å©š|ç¦»å¼€|æŠ›å¼ƒ|æ”¾å¼ƒ|ç–è¿œ|å†·æ·¡|å¿½è§†|èº²é¿|é€ƒé¿|æ‹’ç»|æ’æ–¥|å«‰å¦’|ç¾¡æ…•|æ§åˆ¶|å æœ‰|æŸç¼š|å‹åŠ›|çƒ¦æ¼|å›°æ‰°|ç—›è‹¦|ä¼¤å¿ƒ|éš¾è¿‡|å¤±æœ›|ç»æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]"
    ]

    # é‡‡æ ·æ•°æ®é›†çš„å‰sample_sizeè¡Œ
    with open(dataset_path, 'r', encoding='utf-8') as f:
        line_count = 0
        for line in f:
            if line_count >= sample_size:
                break

            try:
                data = json.loads(line)
                text = data.get('text', '').lower()

                # è¯†åˆ«ä¸»é¢˜ - ä¼˜åŒ–é€»è¾‘
                topic = "general"

                # 1. æ£€æŸ¥æ˜¯å¦æƒ…æ„Ÿå¯¹è¯ (æ›´ä¸¥æ ¼çš„æ¡ä»¶)
                is_emotional = False
                for pattern in emotional_patterns:
                    if re.search(pattern, text):
                        is_emotional = True
                        break

                # 2. æ£€æŸ¥å…¶ä»–ä¸»é¢˜
                found_other_topic = False
                for topic_name, keywords in topic_keywords.items():
                    # è·³è¿‡æƒ…æ„Ÿä¸»é¢˜ï¼Œå•ç‹¬å¤„ç†
                    if topic_name == "emotional":
                        continue

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¯¥ä¸»é¢˜çš„å…³é”®è¯
                    if any(keyword in text for keyword in keywords):
                        topic = topic_name
                        found_other_topic = True
                        break

                # 3. å¦‚æœæ²¡æœ‰å…¶ä»–ä¸»é¢˜ä½†ç¬¦åˆæƒ…æ„Ÿå¯¹è¯æ¡ä»¶
                if not found_other_topic and is_emotional:
                    topic = "emotional"

                topic_sequence.append(topic)
                topic_count[topic] += 1
                line_count += 1
            except:
                continue

    # è®¡ç®—åŒä¸»é¢˜è¿ç»­å‡ºç°æ¬¡æ•°
    max_streak = 0
    current_streak = 0
    current_topic = None

    for topic in topic_sequence:
        if topic == current_topic:
            current_streak += 1
            max_streak = max(max_streak, current_streak)
        else:
            current_streak = 1
            current_topic = topic

    # è®¡ç®—ä¸»é¢˜è½¬æ¢é¢‘ç‡
    topic_changes = 0
    if len(topic_sequence) > 1:
        topic_changes = sum(1 for i in range(1, len(topic_sequence))
                            if topic_sequence[i] != topic_sequence[i - 1])
    change_rate = topic_changes / len(topic_sequence) if topic_sequence else 0

    # è®¡ç®—ä¸»é¢˜åˆ†å¸ƒç†µï¼ˆè¡¡é‡å¤šæ ·æ€§ï¼‰
    total = len(topic_sequence)
    probs = [count / total for count in topic_count.values()] if total > 0 else []
    entropy = -sum(p * math.log(p + 1e-10) for p in probs) if probs else 0

    # æƒ…æ„Ÿå¯¹è¯åˆ†å¸ƒæŠ¥å‘Š
    emotional_percent = topic_count.get("emotional", 0) / total * 100 if total > 0 else 0

    print(f"æœ€å¤§è¿ç»­ç›¸åŒä¸»é¢˜: {max_streak}")
    print(f"ä¸»é¢˜è½¬æ¢é¢‘ç‡: {change_rate:.4f} (æ¯è¡Œ)")
    print(f"ä¸»é¢˜åˆ†å¸ƒç†µ: {entropy:.4f}")
    print(f"æƒ…æ„Ÿå¯¹è¯å æ¯”: {emotional_percent:.2f}%")
    print(f"ä¸»é¢˜åˆ†å¸ƒ: {dict(topic_count)}")

    # è¯„ä¼°æ ‡å‡†ï¼ˆå¢åŠ æƒ…æ„Ÿå¯¹è¯æƒé‡ï¼‰
    quality_score = 0

    # 1. æœ€å¤§è¿ç»­ä¸»é¢˜è¡Œæ•°
    if max_streak <= 10:
        quality_score += 2
    elif max_streak <= 20:
        quality_score += 1

    # 2. ä¸»é¢˜è½¬æ¢é¢‘ç‡
    if change_rate >= 0.5:
        quality_score += 2
    elif change_rate >= 0.3:
        quality_score += 1

    # 3. ä¸»é¢˜åˆ†å¸ƒç†µ
    if entropy >= 1.5:
        quality_score += 1
    elif entropy >= 1.0:
        quality_score += 0.5

    # 4. æƒ…æ„Ÿå¯¹è¯åˆ†å¸ƒ
    if 5 <= emotional_percent <= 20:  # ç†æƒ³å æ¯”èŒƒå›´
        quality_score += 1
    elif emotional_percent > 0:  # è‡³å°‘æœ‰ä¸€å®šæ¯”ä¾‹
        quality_score += 0.5

    # è¾“å‡ºè´¨é‡æŠ¥å‘Š
    if quality_score >= 5:
        print("âœ… æ•°æ®é›†å·²å……åˆ†æ‰“ä¹± (è´¨é‡è¯„åˆ†: ä¼˜ç§€)")
        return True
    elif quality_score >= 4:
        print("âš ï¸ æ•°æ®é›†æ‰“ä¹±åŸºæœ¬åˆæ ¼ (è´¨é‡è¯„åˆ†: è‰¯å¥½)")
        return True
    elif quality_score >= 3:
        print("âš ï¸ æ•°æ®é›†æ‰“ä¹±å‹‰å¼ºåˆæ ¼ (è´¨é‡è¯„åˆ†: ä¸­ç­‰)")
        return True
    else:
        print("âŒ æ•°æ®é›†æ‰“ä¹±ä¸å……åˆ† (è´¨é‡è¯„åˆ†: å·®)")
        return False


def shuffle_large_jsonl(input_path, output_path, buffer_size=100000, max_attempts=3):
    """æ‰“ä¹±å¤§å‹JSONLæ•°æ®é›†ï¼Œå¹¶è‡ªåŠ¨éªŒè¯æ‰“ä¹±è´¨é‡"""
    input_file = Path(input_path)
    output_file = Path(output_path)

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = output_file.parent / f"temp_shuffle_{int(time.time())}"
    temp_dir.mkdir(exist_ok=True)

    attempt = 1
    shuffle_quality = False

    while attempt <= max_attempts and not shuffle_quality:
        print(f"\n=== æ‰“ä¹±å°è¯• #{attempt}/{max_attempts} ===")

        # ç¬¬ä¸€æ­¥ï¼šç»Ÿè®¡æ€»è¡Œæ•°
        print("â–¶ ç»Ÿè®¡æ•°æ®é›†è¡Œæ•°...")
        total_lines = 0
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                total_lines += 1

        # ç¬¬äºŒæ­¥ï¼šåˆ›å»ºè¡Œç´¢å¼•
        print("â–¶ åˆ›å»ºè¡Œç´¢å¼•...")
        line_indices = list(range(total_lines))
        random.shuffle(line_indices)

        # ç¬¬ä¸‰æ­¥ï¼šåˆ†å—æ‰“ä¹±
        print("â–¶ åˆ†å—æ‰“ä¹±æ•°æ®...")
        buffer = []
        chunk_count = 0
        with open(input_file, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f):
                # è·³è¿‡ç©ºè¡Œ
                if not line.strip():
                    continue

                buffer.append((line_indices[idx], line))

                if len(buffer) >= buffer_size:
                    # ä½¿ç”¨é«˜æ•ˆæ’åºç®—æ³•
                    buffer.sort(key=lambda x: x[0])
                    chunk_path = temp_dir / f"chunk_{chunk_count}.jsonl"
                    with open(chunk_path, 'w', encoding='utf-8') as chunk_f:
                        for _, line_text in buffer:
                            chunk_f.write(line_text)
                    chunk_count += 1
                    buffer = []

        # å¤„ç†å‰©ä½™è¡Œ
        if buffer:
            buffer.sort(key=lambda x: x[0])
            chunk_path = temp_dir / f"chunk_{chunk_count}.jsonl"
            with open(chunk_path, 'w', encoding='utf-8') as chunk_f:
                for _, line_text in buffer:
                    chunk_f.write(line_text)
            chunk_count += 1

        # ç¬¬å››æ­¥ï¼šåˆå¹¶åˆ†å—
        print("â–¶ åˆå¹¶æ‰“ä¹±åçš„æ•°æ®...")
        with open(output_file, 'w', encoding='utf-8') as out_f:
            for i in range(chunk_count):
                chunk_path = temp_dir / f"chunk_{i}.jsonl"
                with open(chunk_path, 'r', encoding='utf-8') as chunk_f:
                    shutil.copyfileobj(chunk_f, out_f)
                chunk_path.unlink()

        print(f"âœ… æ•°æ®é›†å·²æ‰“ä¹±å¹¶ä¿å­˜è‡³: {output_path}")

        # ç¬¬äº”æ­¥ï¼šéªŒè¯æ‰“ä¹±è´¨é‡
        print("â–¶ éªŒè¯æ‰“ä¹±è´¨é‡...")
        shuffle_quality = validate_shuffle(output_file)

        if not shuffle_quality and attempt < max_attempts:
            print(f"âš ï¸ æ‰“ä¹±è´¨é‡ä¸åˆæ ¼ï¼Œå°†å°è¯•é‡æ–°æ‰“ä¹±...")

        attempt += 1

    # æ¸…ç†ä¸´æ—¶ç›®å½•
    try:
        shutil.rmtree(temp_dir)
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•æ—¶å‡ºé”™: {str(e)}")

    return shuffle_quality


if __name__ == "__main__":
    # æ‰“ä¹±åˆå¹¶æ•°æ®é›†
    success = shuffle_large_jsonl(
        input_path=r"D:\PythonCode\RainLLM\dataset\pretrain_combined.jsonl",
        output_path=r"D:\PythonCode\RainLLM\dataset\pretrain_combined_shuffled.jsonl",
        buffer_size=100000,
        max_attempts=5
    )

    if success:
        print("\nğŸ‰ æ•°æ®é›†æ‰“ä¹±æˆåŠŸä¸”è´¨é‡åˆæ ¼")
    else:
        print("\nâš ï¸ è­¦å‘Šï¼šæ•°æ®é›†æ‰“ä¹±åè´¨é‡ä»ä¸åˆæ ¼ï¼Œå»ºè®®æ‰‹åŠ¨æ£€æŸ¥")

    # æ·»åŠ æƒ…æ„Ÿå¯¹è¯åˆ†å¸ƒæŠ¥å‘Š
    print("\næƒ…æ„Ÿå¯¹è¯åˆ†å¸ƒåˆ†æ:")
    print("1. æƒ…æ„Ÿå¯¹è¯åº”å æ€»æ•°æ®çš„5-20%")
    print("2. æƒ…æ„Ÿå¯¹è¯åº”å‡åŒ€åˆ†å¸ƒåœ¨æ•°æ®é›†ä¸­")
    print("3. é¿å…å¤§æ®µè¿ç»­çš„æƒ…æ„Ÿå¯¹è¯å†…å®¹")