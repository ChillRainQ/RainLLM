
import json
import os
import random
import re
import multiprocessing
from collections import defaultdict
from tqdm import tqdm
import argparse

SPECIAL_TOKENS = ["<|endoftext|>", "<|im_start|>", "<|im_end|>", "<|system|>", "<|user|>", "<|assistant|>"]


class AdvancedTopicClassifier:
    def __init__(self):
        self.topic_keywords = {
            "medical": ["åŒ»å­¦", "åŒ»ç–—", "å¥åº·", "ç–¾ç—…", "åŒ»é™¢", "åŒ»ç”Ÿ", "æ‚£è€…", "æ²»ç–—", "è¯Šæ–­", "è¯ç‰©", "æ‰‹æœ¯"],
            "code": ["ä»£ç ", "ç¼–ç¨‹", "å‡½æ•°", "å˜é‡", "ç±»", "æ–¹æ³•", "ç®—æ³•", "æ•°æ®åº“", "API", "ç¼–ç¨‹è¯­è¨€", "è°ƒè¯•", "æ¡†æ¶"],
            "finance": ["é‡‘è", "è‚¡ç¥¨", "æŠ•èµ„", "é“¶è¡Œ", "ç»æµ", "å¸‚åœº", "äº¤æ˜“", "è´§å¸", "æ±‡ç‡", "åŸºé‡‘", "ç†è´¢"],
            "education": ["æ•™è‚²", "å­¦æ ¡", "å­¦ä¹ ", "è¯¾ç¨‹", "æ•™å­¦", "è€å¸ˆ", "å­¦ç”Ÿ", "è€ƒè¯•", "åŸ¹è®­", "æ•™æ", "å­¦ä½"],
            "tech": ["æŠ€æœ¯", "ç§‘æŠ€", "åˆ›æ–°", "ç ”å‘", "å·¥ç¨‹", "è®¾å¤‡", "ç³»ç»Ÿ", "åº”ç”¨", "æ™ºèƒ½", "æœºå™¨äºº", "äººå·¥æ™ºèƒ½"],
            "entertainment": ["å¨±ä¹", "ç”µå½±", "éŸ³ä¹", "æ¸¸æˆ", "æ˜æ˜Ÿ", "èŠ‚ç›®", "æ¼”å‡º", "ç»¼è‰º", "è‰ºäºº", "æ¼”å”±ä¼š",
                              "ç”µè§†å‰§"],
            "emotional": ["æƒ…æ„Ÿ", "å¿ƒæƒ…", "æ„Ÿå—", "çˆ±", "å–œæ¬¢", "æ‹çˆ±", "åˆ†æ‰‹", "å®‰æ…°", "æ”¯æŒ", "ç†è§£", "å…³ç³»",
                          "æœ‹å‹", "å®¶äºº", "å­¤ç‹¬", "å¼€å¿ƒ", "ä¼¤å¿ƒ", "æ„¤æ€’", "å‹åŠ›", "å¿ƒç†å’¨è¯¢", "æƒ…ç»ªç®¡ç†", "å…±æƒ…",
                          "äº²å¯†å…³ç³»", "æƒ…æ„Ÿè¡¨è¾¾", "å¿ƒç†æ”¯æŒ", "æƒ…æ„Ÿéœ€æ±‚", "æƒ…æ„Ÿå›°æƒ‘", "æƒ…æ„Ÿäº¤æµ", "æƒ…æ„Ÿå’¨è¯¢"],
            "science": ["ç§‘å­¦", "ç‰©ç†", "åŒ–å­¦", "ç”Ÿç‰©", "æ•°å­¦", "å¤©æ–‡", "åœ°ç†", "å®éªŒ", "ç†è®º", "ç ”ç©¶", "è‡ªç„¶"],
            "literature": ["æ–‡å­¦", "å°è¯´", "è¯—æ­Œ", "æ•£æ–‡", "ä½œå®¶", "ä½œå“", "é˜…è¯»", "å†™ä½œ", "æ•…äº‹", "æƒ…èŠ‚", "äººç‰©"],
            "sports": ["ä½“è‚²", "è¿åŠ¨", "æ¯”èµ›", "è¶³çƒ", "ç¯®çƒ", "è¿åŠ¨å‘˜", "è®­ç»ƒ", "å¥¥è¿", "å† å†›", "èµ›äº‹", "å¥èº«"]
        }

        # é¢„ç¼–è¯‘æƒ…æ„Ÿæ­£åˆ™ï¼ˆå¸¦ç‰¹æ®Štokenä¿æŠ¤ï¼‰
        self.emotional_patterns = [
            re.compile(
                r"(?<!<\|)(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬|ä½ ä»¬|ä»–ä»¬|å¥¹ä»¬|å¤§å®¶|æœ‰äºº|æŸäºº)?(æ„Ÿåˆ°|è§‰å¾—|æ„Ÿè§‰|è®¤ä¸º|ä»¥ä¸º|å‘ç°|çŸ¥é“|å¸Œæœ›|æƒ³è¦|éœ€è¦|å–œæ¬¢|çˆ±|è®¨åŒ|æ¨|å®³æ€•|æ‹…å¿ƒ|ç„¦è™‘|ç”Ÿæ°”|æ„¤æ€’|ä¼¤å¿ƒ|éš¾è¿‡|å¼€å¿ƒ|å¿«ä¹|å…´å¥‹|æƒŠå–œ|å‹åŠ›å¤§|å­¤ç‹¬|å¯‚å¯|æ²®ä¸§|å¤±æœ›|æ— åŠ©|å›°æƒ‘|çŸ›ç›¾|çº ç»“|çŠ¹è±«|çƒ¦æ¼|éƒé—·|å§”å±ˆ|å«‰å¦’|ç¾¡æ…•|å†…ç–š|åæ‚”|ç¾æ„§|è‡ªè±ª|æ»¡è¶³|æ”¾æ¾|å¹³é™|å®‰å¿ƒ|æ„Ÿæ¿€|æ„Ÿæ©|æ„ŸåŠ¨|æ¸©æš–|å¹¸ç¦|èˆ’æœ|è‡ªåœ¨|è‡ªä¿¡|ä¹è§‚|æ‚²è§‚|æ¶ˆæ|ç§¯æ|ç´§å¼ |ç–²åŠ³|ç´¯|å›°|é¥¿|æ¸´|ç—›|ç—’|å†·|çƒ­|ä¸èˆ’æœ)?[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|å¿ƒæƒ…|æ„Ÿå—|æƒ…ç»ª|æ‹çˆ±|åˆ†æ‰‹|å¤åˆ|åµæ¶|å’Œè§£|é“æ­‰|åŸè°…|èƒŒå›|ä¿¡ä»»|çŒœç–‘|äº²å¯†|å†²çª|å®¶åº­|æœ‹å‹|çˆ±æƒ…|å­¤ç‹¬|å…³ç³»|è¡¨è¾¾|å€¾è¯‰)(?!\|>)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]"),
            re.compile(
                r"(?<!<\|)(å¦‚ä½•|æ€æ ·|æ€ä¹ˆ|ä»€ä¹ˆ|ä¸ºä»€ä¹ˆ|æ˜¯ä¸æ˜¯|èƒ½å¦|è¦ä¸è¦|è¯¥ä¸è¯¥|å€¼ä¸å€¼|æœ‰æ²¡æœ‰|èƒ½ä¸èƒ½|ä¼šä¸ä¼š|æƒ³ä¸æƒ³|æ„¿ä¸æ„¿)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(å¤„ç†|è§£å†³|é¢å¯¹|è¡¨è¾¾|æ²Ÿé€š|æ”¹å–„|ä¿®å¤|çæƒœ|å¿˜è®°|æ€€å¿µ|å›å¿†)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(æƒ…æ„Ÿ|æ‹çˆ±|å…³ç³»|å®¶åº­|çˆ¶æ¯|æœ‹å‹|å©šå§»|çˆ±æƒ…)(?!\|>)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]"),
            re.compile(
                r"(?<!<\|)(æˆ‘|ä½ |ä»–|å¥¹|æˆ‘ä»¬)?(çš„)?(ç”·å‹|å¥³æœ‹å‹|è€å©†|ä¸ˆå¤«|ä¼´ä¾£|å‰ä»»|çˆ¶æ¯|æœ‹å‹|æ‹äºº|å¯¹è±¡|è€å¸ˆ|å­©å­)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?(ä¸ç†|ä¼¤å®³|æ¬ºéª—|åµæ¶|åˆ†æ‰‹|å†·æˆ˜|å¿½è§†|ç¦»å©š|å‹åŠ›|ä¼¤å¿ƒ|çº ç»“|å›°æƒ‘)(?!\|>)[^ã€‚ï¼Œï¼›ï¼ï¼Ÿ]*?[ã€‚ï¼Œï¼›ï¼ï¼Ÿ]")
        ]

    def classify(self, text):
        # ç‰¹æ®Štokenä¿æŠ¤ï¼ˆä¸´æ—¶æ›¿æ¢ï¼‰
        protected_text = text
        placeholder_map = {}
        for i, token in enumerate(SPECIAL_TOKENS):
            placeholder = f"__SPECIAL_{i}__"
            protected_text = protected_text.replace(token, placeholder)
            placeholder_map[placeholder] = token

        # æƒ…æ„Ÿç±»æ£€æµ‹ï¼ˆä½¿ç”¨æ­£åˆ™ï¼‰
        lower_text = protected_text.lower()
        for pattern in self.emotional_patterns:
            if pattern.search(lower_text):
                return "emotional"

        # å…¶ä»–ä¸»é¢˜æ£€æµ‹
        for topic, keywords in self.topic_keywords.items():
            if topic == "emotional":
                continue
            if any(kw in protected_text for kw in keywords):
                return topic

        # æ¢å¤åŸå§‹ç‰¹æ®Štoken
        for placeholder, token in placeholder_map.items():
            text = text.replace(placeholder, token)

        return "general"


def process_line(line, target_topics):
    try:
        data = json.loads(line)
        text = data.get("text", "")

        # è®°å½•åŸå§‹ç‰¹æ®Štoken
        original_tokens = {token: text.count(token) for token in SPECIAL_TOKENS}

        # åˆ†ç±»
        classifier = AdvancedTopicClassifier()
        topic = data.get("topic") or classifier.classify(text)

        # éªŒè¯å¹¶æ¢å¤ç‰¹æ®Štoken
        if "text" in data:
            for token, count in original_tokens.items():
                if data["text"].count(token) != count:
                    # ä¿®å¤è¢«ä¿®æ”¹çš„token
                    data["text"] = data["text"].replace(token.lower(), token)
                    for special in SPECIAL_TOKENS:
                        if special != token:
                            data["text"] = data["text"].replace(special.lower(), special)

        return (topic if topic in target_topics else "others"), data
    except:
        return None, None


def worker(chunk, target_topics):
    local_result = defaultdict(list)
    for line in chunk:
        topic, data = process_line(line, target_topics)
        if topic:
            local_result[topic].append(data)
    return local_result


def main():
    parser = argparse.ArgumentParser(description='é«˜çº§æ•°æ®é‡é‡‡æ ·å™¨')
    parser.add_argument("--input", required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", required=True, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--ratios", type=str,
                        default="emotional:0.2,code:0.2,general:0.3,medical:0.1,tech:0.05,finance:0.05,education:0.05,science:0.03,entertainment:0.02,literature:0.02,sports:0.01,others:0.03",
                        help="ç›®æ ‡æ¯”ä¾‹ï¼Œæ ¼å¼: topic1:ratio1,topic2:ratio2,...")
    parser.add_argument("--threads", type=int, default=multiprocessing.cpu_count())
    parser.add_argument("--min_samples", type=int, default=1000)
    parser.add_argument("--strict_mode", action="store_true", help="ä¸¥æ ¼æ¨¡å¼ï¼ˆæ¯”ä¾‹ä¸è¶³æ—¶æŠ¥é”™ï¼‰")
    parser.add_argument("--verify_tokens", action="store_true", help="æ‰§è¡Œç‰¹æ®ŠtokenéªŒè¯")
    parser.add_argument("--clip_ratio", type=float, default=0.0,
                        help="å…¨å±€æ•°æ®è£å‰ªæ¯”ä¾‹ï¼Œä¾‹å¦‚0.3è¡¨ç¤ºè£å‰ª30%%æ•°æ®ï¼Œä¿æŒå„ç±»åˆ«å‡åŒ€è£å‰ª")
    args = parser.parse_args()

    # è§£æç›®æ ‡æ¯”ä¾‹
    target_ratios = {k.strip(): float(v) for k, v in
                     (item.split(':') for item in args.ratios.split(','))}
    required_topics = [k for k in target_ratios.keys() if k != "others"]

    # è¯»å–æ•°æ®
    print(f"âŒ› æ­£åœ¨åŠ è½½ {os.path.basename(args.input)}...")
    with open(args.input, "r", encoding="utf-8") as f:
        lines = f.readlines()
    total_lines = len(lines)
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {total_lines:,}")

    # å¤šè¿›ç¨‹å¤„ç†
    print(f"âš¡ ä½¿ç”¨ {args.threads} ä¸ªçº¿ç¨‹è¿›è¡Œåˆ†ç±»å¤„ç†...")
    chunk_size = len(lines) // args.threads + 1
    chunks = [lines[i:i + chunk_size] for i in range(0, len(lines), chunk_size)]

    with multiprocessing.Pool(args.threads) as pool:
        results = pool.starmap(worker, [(chunk, required_topics) for chunk in chunks])

    # åˆå¹¶ç»“æœ
    classified_data = defaultdict(list)
    for r in results:
        for k, v in r.items():
            classified_data[k].extend(v)

    # ç»Ÿè®¡åˆ†å¸ƒ
    total_classified = sum(len(v) for v in classified_data.values())
    print("\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
    for topic in sorted(classified_data.keys(), key=lambda x: -len(classified_data[x])):
        print(f"  {topic}: {len(classified_data[topic]):>8,} ({len(classified_data[topic]) / total_classified:>6.1%})")

    # è®¡ç®—ä¿ç•™æ¯”ä¾‹
    clip_ratio = args.clip_ratio
    keep_ratio = 1.0 - clip_ratio
    if clip_ratio > 0:
        print(f"\nâœ‚ï¸ å…¨å±€è£å‰ªæ¯”ä¾‹: {clip_ratio*100:.1f}%ï¼Œå„ç±»åˆ«å‡åŒ€è£å‰ª")

    # æ¯”ä¾‹é‡‡æ ·ï¼ˆå«å‡åŒ€è£å‰ªï¼‰
    print("\nğŸ”§ æ‰§è¡Œæ¯”ä¾‹é‡‡æ ·:")
    final_data = []
    for topic, ratio in target_ratios.items():
        if topic == "others":
            continue

        available = len(classified_data.get(topic, []))
        # åŸå§‹ç›®æ ‡æ ·æœ¬æ•°
        target_orig = max(int(total_classified * ratio), args.min_samples)
        # è£å‰ªåç›®æ ‡æ•°
        target = int(target_orig * keep_ratio)
        # ä¸è¶…è¿‡available
        if available < target:
            msg = f"  âœ– {topic}: éœ€è¦ {target:,} ä½†åªæœ‰ {available:,}"
            if args.strict_mode:
                raise ValueError(msg + " (ä¸¥æ ¼æ¨¡å¼å¯ç”¨)")
            else:
                print(msg + " (ä½¿ç”¨å¯ç”¨æ ·æœ¬)")
                target = available

        if target <= 0:
            continue

        sampled = random.sample(classified_data[topic], target)
        final_data.extend(sampled)
        print(f"  âœ” {topic}: {len(sampled):>7,}/{available:,}")

    # å¤„ç†othersç±»åˆ«ï¼ˆå«è£å‰ªï¼‰
    if "others" in target_ratios:
        other_topics = set(classified_data.keys()) - set(target_ratios.keys())
        other_samples = []
        for topic in other_topics:
            other_samples.extend(classified_data[topic])

        target_orig = max(int(total_classified * target_ratios["others"]), args.min_samples)
        target = int(target_orig * keep_ratio)
        target = min(target, len(other_samples))

        if target > 0:
            sampled = random.sample(other_samples, target)
            final_data.extend(sampled)
            print(f"  âœ” others: {len(sampled):>7,}/{len(other_samples):,}")

    # æ‰“ä¹±å¹¶ä¿å­˜
    random.shuffle(final_data)
    print(f"\nğŸ’¾ å†™å…¥ {len(final_data):,} æ¡æ•°æ®åˆ° {args.output}")
    with open(args.output, "w", encoding="utf-8") as f:
        for item in tqdm(final_data, desc="ä¿å­˜è¿›åº¦"):
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    # ç‰¹æ®ŠtokenéªŒè¯
    if args.verify_tokens and final_data:
        print("\nğŸ” ç‰¹æ®ŠtokenéªŒè¯:")
        sample = final_data[0]
        token_status = {token: "âœ”" if token in sample.get("text", "") else "âœ–" for token in SPECIAL_TOKENS}
        for token, status in token_status.items():
            print(f"  {token}: {status}")

        error_count = 0
        for item in random.sample(final_data, min(100, len(final_data))):
            text = item.get("text", "")
            for token in SPECIAL_TOKENS:
                if token in text and token.lower() in text.lower() and token != token.lower():
                    error_count += 1
        print(f"  å‘ç° {error_count} ä¸ªæ½œåœ¨çš„å¤§å°å†™è½¬æ¢é—®é¢˜")

    # æœ€ç»ˆæ¯”ä¾‹æŠ¥å‘Š
    dist = defaultdict(int)
    for item in final_data:
        topic = item.get("topic", "others")
        if topic not in target_ratios:
            topic = "others"
        dist[topic] += 1

    print("\nğŸ¯ æœ€ç»ˆåˆ†å¸ƒ:")
    for topic in sorted(dist.keys(), key=lambda x: -dist[x]):
        print(f"  {topic}: {dist[topic] / len(final_data):.1%} (ç›®æ ‡: {target_ratios.get(topic, 0):.0%})")


if __name__ == "__main__":
    print("=" * 60)
    print("é«˜çº§æ•°æ®é‡é‡‡æ ·å™¨ v2.1".center(60))
    print("=" * 60)
    main()
